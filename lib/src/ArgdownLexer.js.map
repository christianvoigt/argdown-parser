{"version":3,"sources":["../../src/ArgdownLexer.js"],"names":["chevrotain","_","createToken","Lexer","indentStack","getCurrentLine","matchedTokens","matchedTokensIsEmpty","isEmpty","last","currentLine","getEndLine","tokenMatcher","Emptyline","emitRemainingDedentTokens","length","lastToken","tokens","lastOffset","getEndOffset","lastLine","lastColumn","getEndColumn","push","Dedent","pop","emitIndentOrDedent","groups","indentStr","currIndentLevel","lastIndentLevel","image","offset","line","nl","column","indentToken","Indent","dedentToken","matchRelation","text","pattern","startsWithNewline","exec","match","matchIncomingSupport","partialRight","matchIncomingAttack","matchOutgoingSupport","matchOutgoingAttack","IncomingSupport","name","IncomingAttack","OutgoingSupport","OutgoingAttack","emptylinePattern","emptylineMatching","NA","StatementDefinition","StatementReference","ArgumentDefinition","ArgumentReference","Newline","group","SKIPPED","Spaces","Freestyle","allTokens","customPatternLexer","module","exports","logTokens","lexResult","token","console","log","constructor","tokenize","errors","Error"],"mappings":"AAAA;;AAEA;;IAAYA,U;;AACZ;;IAAYC,C;;;;AAEZ,IAAMC,cAAcF,WAAWE,WAA/B;AACA,IAAMC,QAAQH,WAAWG,KAAzB;;AAEA;AACA,IAAIC,cAAc,CAAC,CAAD,CAAlB;;AAEA,SAASC,cAAT,CAAwBC,aAAxB,EAAsC;AACpC,MAAMC,uBAAuBN,EAAEO,OAAF,CAAUF,aAAV,CAA7B;AACA,MAAGC,oBAAH,EACE,OAAO,CAAP;;AAEF,MAAIE,OAAOR,EAAEQ,IAAF,CAAOH,aAAP,CAAX;AACA,MAAII,cAAcV,WAAWW,UAAX,CAAsBF,IAAtB,CAAlB;AACA,MAAGT,WAAWY,YAAX,CAAwBH,IAAxB,EAA8BI,SAA9B,CAAH,EACEH;AACF,SAAOA,WAAP;AACD;AACD,SAASI,yBAAT,CAAmCR,aAAnC,EAAiD;AAC/C,MAAGF,YAAYW,MAAZ,IAAsB,CAAzB,EACE;AACF,MAAMC,YAAYf,EAAEQ,IAAF,CAAOH,cAAcW,MAArB,CAAlB;AACA,MAAMC,aAAcF,SAAD,GAAahB,WAAWmB,YAAX,CAAwBH,SAAxB,CAAb,GAAkD,CAArE;AACA,MAAMI,WAAYJ,SAAD,GAAahB,WAAWW,UAAX,CAAsBK,SAAtB,CAAb,GAAgD,CAAjE;AACA,MAAMK,aAAcL,SAAD,GAAahB,WAAWsB,YAAX,CAAwBN,SAAxB,CAAb,GAAkD,CAArE;;AAEA;AACA,SAAOZ,YAAYW,MAAZ,GAAqB,CAA5B,EAA+B;AAC3BT,kBAAciB,IAAd,CAAmB,IAAIC,MAAJ,CAAW,EAAX,EAAeN,UAAf,EAA2BE,QAA3B,EAAqCC,UAArC,CAAnB;AACAjB,gBAAYqB,GAAZ;AACH;AACF;;AAED,SAASC,kBAAT,CAA4BpB,aAA5B,EAA2CqB,MAA3C,EAAmDC,SAAnD,EAA6D;AAC3D,MAAMC,kBAAkBD,UAAUb,MAAlC;AACA,MAAMe,kBAAkB7B,EAAEQ,IAAF,CAAOL,WAAP,CAAxB;AACA,MAAM2B,QAAQ,EAAd;AACA,MAAMtB,OAAOR,EAAEQ,IAAF,CAAOH,aAAP,CAAb;AACA,MAAM0B,SAAUvB,IAAD,GAAQT,WAAWmB,YAAX,CAAwBV,IAAxB,IAAgC,CAAxC,GAA4C,CAA3D;AACA,MAAMwB,OAAO5B,eAAeC,aAAf,EAA8BqB,OAAOO,EAArC,CAAb;AACA,MAAMC,SAAU1B,IAAD,GAAQT,WAAWsB,YAAX,CAAwBb,IAAxB,IAAgC,CAAxC,GAA4C,CAA3D;AACA,MAAGoB,kBAAkBC,eAArB,EAAqC;AACnC1B,gBAAYmB,IAAZ,CAAiBM,eAAjB;AACA,QAAIO,cAAc,IAAIC,MAAJ,CAAWN,KAAX,EAAkBC,MAAlB,EAA0BC,IAA1B,EAAgCE,MAAhC,CAAlB;AACA7B,kBAAciB,IAAd,CAAmBa,WAAnB;AACD,GAJD,MAIM,IAAGP,kBAAkBC,eAArB,EAAqC;AACzC,WAAM1B,YAAYW,MAAZ,GAAqB,CAArB,IAA0Bc,kBAAkB5B,EAAEQ,IAAF,CAAOL,WAAP,CAAlD,EAAsE;AACpEA,kBAAYqB,GAAZ;AACA,UAAIa,cAAc,IAAId,MAAJ,CAAWO,KAAX,EAAkBC,MAAlB,EAA0BC,IAA1B,EAAgCE,MAAhC,CAAlB;AACA7B,oBAAciB,IAAd,CAAmBe,WAAnB;AACD;AACF;AACF;;AAED,SAASC,aAAT,CAAuBC,IAAvB,EAA6BlC,aAA7B,EAA4CqB,MAA5C,EAAoDc,OAApD,EAA4D;AAC1D,MAAIC,oBAAoB,gBAAgBC,IAAhB,CAAqBH,IAArB,KAA8B,IAAtD;AACA,MAAIvC,EAAEO,OAAF,CAAUF,aAAV,KAA4BoC,iBAAhC,EAAmD;AACjD,QAAIE,QAAQH,QAAQE,IAAR,CAAaH,IAAb,CAAZ;AACA,QAAGI,UAAU,IAAV,IAAkBA,MAAM7B,MAAN,IAAgB,CAArC,EAAuC;AACrC,UAAIa,YAAYgB,MAAM,CAAN,CAAhB;AACAlB,yBAAmBpB,aAAnB,EAAkCqB,MAAlC,EAA0CC,SAA1C;AACA,aAAOgB,KAAP;AACD;AACF;AACD,SAAO,IAAP;AACD;AACD;AACA,IAAIC,uBAAuB5C,EAAE6C,YAAF,CAAeP,aAAf,EAA8B,+BAA9B,CAA3B;AACA,IAAIQ,sBAAsB9C,EAAE6C,YAAF,CAAeP,aAAf,EAA8B,8BAA9B,CAA1B;AACA,IAAIS,uBAAuB/C,EAAE6C,YAAF,CAAeP,aAAf,EAA8B,gCAA9B,CAA3B;AACA,IAAIU,sBAAsBhD,EAAE6C,YAAF,CAAeP,aAAf,EAA8B,+BAA9B,CAA1B;;AAEA,IAAIW,kBAAkBhD,YAAY;AAC9BiD,QAAM,iBADwB;AAE9BV,WAASI;AAFqB,CAAZ,CAAtB;;AAKA,IAAIO,iBAAiBlD,YAAY;AAC7BiD,QAAM,gBADuB;AAE7BV,WAASM;AAFoB,CAAZ,CAArB;;AAKA,IAAIM,kBAAkBnD,YAAY;AAC9BiD,QAAM,iBADwB;AAE9BV,WAASO;AAFqB,CAAZ,CAAtB;;AAKA,IAAIM,iBAAiBpD,YAAY;AAC7BiD,QAAM,gBADuB;AAE7BV,WAASQ;AAFoB,CAAZ,CAArB;;AAKA,IAAMM,mBAAmB,4BAAzB,C,CAAuD;AACvD,SAASC,iBAAT,CAA2BhB,IAA3B,EAAiClC,aAAjC,EAA+C;AAC7C,MAAIsC,QAAQW,iBAAiBZ,IAAjB,CAAsBH,IAAtB,CAAZ;AACA,MAAGI,UAAU,IAAV,IAAkBA,MAAM7B,MAAN,IAAgB,CAArC,EAAuC;AACrCD,8BAA0BR,aAA1B;AACAsC,UAAM,CAAN,IAAWA,MAAM,CAAN,CAAX;AACA,WAAOA,KAAP;AACD;AACD,SAAO,IAAP;AACD;AACD,IAAI/B,YAAYX,YAAY;AACxBiD,QAAM,WADkB;AAExBV,WAASe;AAFe,CAAZ,CAAhB;;AAKA;AACA,IAAInB,SAASnC,YAAY;AACrBiD,QAAM,QADe;AAErBV,WAAStC,MAAMsD;AAFM,CAAZ,CAAb;AAIA,IAAIjC,SAAStB,YAAY;AACrBiD,QAAM,QADe;AAErBV,WAAStC,MAAMsD;AAFM,CAAZ,CAAb;;AAKA,IAAIC,sBAAsBxD,YAAY;AACpCiD,QAAM,qBAD8B;AAEpCV,WAAS;AAF2B,CAAZ,CAA1B;;AAKA,IAAIkB,qBAAqBzD,YAAY;AACnCiD,QAAM,oBAD6B;AAEnCV,WAAS;AAF0B,CAAZ,CAAzB;;AAKA,IAAImB,qBAAqB1D,YAAY;AACnCiD,QAAM,oBAD6B;AAEnCV,WAAS;AAF0B,CAAZ,CAAzB;;AAKA,IAAIoB,oBAAoB3D,YAAY;AAClCiD,QAAM,mBAD4B;AAElCV,WAAS;AAFyB,CAAZ,CAAxB;;AAKA,IAAIqB,UAAU5D,YAAY;AACtBiD,QAAM,SADgB;AAEtBV,WAAS,SAFa;AAGtBsB,SAAO5D,MAAM6D;AAHS,CAAZ,CAAd;;AAMA,IAAIC,SAAS/D,YAAY;AACrBiD,QAAM,QADe;AAErBV,WAAS,WAFY;AAGrBsB,SAAO5D,MAAM6D;AAHQ,CAAZ,CAAb;AAKA,IAAIE,YAAYhE,YAAY;AAC1BiD,QAAM,WADoB;AAE1BV,WAAS;AAFiB,CAAZ,CAAhB;AAIA,IAAI0B,YAAY,CACZtD,SADY;AAEZ;AACA;AACAW,MAJY,EAKZa,MALY,EAMZa,eANY,EAOZE,cAPY,EAQZC,eARY,EASZC,cATY,EAUZI,mBAVY,EAWZC,kBAXY,EAYZC,kBAZY,EAaZC,iBAbY,EAcZC,OAdY,EAeZG,MAfY,EAgBZC,SAhBY,CAAhB;AAkBA,IAAIE,qBAAqB,IAAIjE,KAAJ,CAAUgE,SAAV,CAAzB;;AAEAE,OAAOC,OAAP,GAAiB;;AAEb;AACAzD,aAAWA,SAHE;AAIbwB,UAAQA,MAJK;AAKbb,UAAQA,MALK;AAMb0B,mBAAiBA,eANJ;AAObE,kBAAgBA,cAPH;AAQbC,mBAAiBA,eARJ;AASbC,kBAAgBA,cATH;AAUbK,sBAAoBA,kBAVP;AAWbD,uBAAqBA,mBAXR;AAYbG,qBAAmBA,iBAZN;AAabD,sBAAoBA,kBAbP;AAcbM,aAAWA,SAdE;AAebD,UAAQA,MAfK;;AAiBbM,aAAW,mBAASC,SAAT,EAAmB;AAAA;AAAA;AAAA;;AAAA;AAC5B,2BAAiBA,UAAUvD,MAA3B,8HAAkC;AAAA,YAA1BwD,KAA0B;;AAChCC,gBAAQC,GAAR,CAAYF,MAAMG,WAAN,CAAkBzB,IAAlB,GAAuB,GAAvB,GAA2BsB,MAAM1C,KAA7C;AACD;AAH2B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAI7B,GArBY;;AAuBbd,UAAQkD,SAvBK;;AAyBbU,YAAU,kBAASrC,IAAT,EAAe;;AAErB;AACApC,kBAAc,CAAC,CAAD,CAAd;;AAEA,QAAIoE,YAAYJ,mBAAmBS,QAAnB,CAA4BrC,IAA5B,CAAhB;;AAEA1B,8BAA0B0D,UAAUvD,MAApC;;AAEA,QAAIuD,UAAUM,MAAV,CAAiB/D,MAAjB,GAA0B,CAA9B,EAAiC;AAC7B,YAAM,IAAIgE,KAAJ,CAAU,sCAAV,CAAN;AACH;AACD,WAAOP,SAAP;AACH;AAtCY,CAAjB","file":"ArgdownLexer.js","sourcesContent":["'use strict';\n\nimport * as chevrotain from 'chevrotain';\nimport * as _ from 'lodash';\n\nconst createToken = chevrotain.createToken;\nconst Lexer = chevrotain.Lexer;\n\n// State required for matching the indentations\nlet indentStack = [0];\n\nfunction getCurrentLine(matchedTokens){\n  const matchedTokensIsEmpty = _.isEmpty(matchedTokens);\n  if(matchedTokensIsEmpty)\n    return 0;\n\n  let last = _.last(matchedTokens);\n  let currentLine = chevrotain.getEndLine(last);\n  if(chevrotain.tokenMatcher(last, Emptyline))\n    currentLine++;\n  return currentLine;\n}\nfunction emitRemainingDedentTokens(matchedTokens){\n  if(indentStack.length <= 1)\n    return;\n  const lastToken = _.last(matchedTokens.tokens);\n  const lastOffset = (lastToken)? chevrotain.getEndOffset(lastToken) : 0;\n  const lastLine = (lastToken)? chevrotain.getEndLine(lastToken) : 0;\n  const lastColumn = (lastToken)? chevrotain.getEndColumn(lastToken) : 0;\n\n  //add remaining Dedents\n  while (indentStack.length > 1) {\n      matchedTokens.push(new Dedent(\"\", lastOffset, lastLine, lastColumn));\n      indentStack.pop();\n  }\n}\n\nfunction emitIndentOrDedent(matchedTokens, groups, indentStr){\n  const currIndentLevel = indentStr.length;\n  const lastIndentLevel = _.last(indentStack);\n  const image = \"\";\n  const last = _.last(matchedTokens);\n  const offset = (last)? chevrotain.getEndOffset(last) + 1 : 0;\n  const line = getCurrentLine(matchedTokens, groups.nl);\n  const column = (last)? chevrotain.getEndColumn(last) + 1 : 0;\n  if(currIndentLevel > lastIndentLevel){\n    indentStack.push(currIndentLevel);\n    let indentToken = new Indent(image, offset, line, column);\n    matchedTokens.push(indentToken);\n  }else if(currIndentLevel < lastIndentLevel){\n    while(indentStack.length > 1 && currIndentLevel < _.last(indentStack)){\n      indentStack.pop();\n      let dedentToken = new Dedent(image, offset, line, column);\n      matchedTokens.push(dedentToken);\n    }\n  }\n}\n\nfunction matchRelation(text, matchedTokens, groups, pattern){\n  let startsWithNewline = /^[\\n\\r|\\n|\\r]/.exec(text) != null;\n  if (_.isEmpty(matchedTokens) || startsWithNewline) {\n    let match = pattern.exec(text);\n    if(match !== null && match.length == 3){\n      let indentStr = match[1];\n      emitIndentOrDedent(matchedTokens, groups, indentStr);\n      return match;\n    }\n  }\n  return null;\n}\n//relations start at BOF or after a newline, optionally followed by indentation (spaces or tabs)\nlet matchIncomingSupport = _.partialRight(matchRelation, /^[\\n\\r|\\n|\\r]?([' '\\t]*)(\\+>)/);\nlet matchIncomingAttack = _.partialRight(matchRelation, /^[\\n\\r|\\n|\\r]?([' '\\t]*)(->)/);\nlet matchOutgoingSupport = _.partialRight(matchRelation, /^[\\n\\r|\\n|\\r]?([' '\\t]*)(<?\\+)/);\nlet matchOutgoingAttack = _.partialRight(matchRelation, /^[\\n\\r|\\n|\\r]?([' '\\t]*)(<?-)/);\n\nlet IncomingSupport = createToken({\n    name: \"IncomingSupport\",\n    pattern: matchIncomingSupport\n});\n\nlet IncomingAttack = createToken({\n    name: \"IncomingAttack\",\n    pattern: matchIncomingAttack\n});\n\nlet OutgoingSupport = createToken({\n    name: \"OutgoingSupport\",\n    pattern: matchOutgoingSupport\n});\n\nlet OutgoingAttack = createToken({\n    name: \"OutgoingAttack\",\n    pattern: matchOutgoingAttack\n});\n\nconst emptylinePattern = /^([\\n\\r|\\n|\\r]{2,})[^\\s\\t]/; //two linebreaks, not followed by whitespace\nfunction emptylineMatching(text, matchedTokens){\n  let match = emptylinePattern.exec(text);\n  if(match !== null && match.length == 2){\n    emitRemainingDedentTokens(matchedTokens);\n    match[0] = match[1];\n    return match;\n  }\n  return null;\n}\nlet Emptyline = createToken({\n    name: \"Emptyline\",\n    pattern: emptylineMatching\n});\n\n//Indent and Dedent are never matched with their own patterns, instead they get matched in the relations custom patterns\nlet Indent = createToken({\n    name: \"Indent\",\n    pattern: Lexer.NA\n});\nlet Dedent = createToken({\n    name: \"Dedent\",\n    pattern: Lexer.NA\n});\n\nlet StatementDefinition = createToken({\n  name: \"StatementDefinition\",\n  pattern: /\\[.+\\]\\:/\n});\n\nlet StatementReference = createToken({\n  name: \"StatementReference\",\n  pattern: /\\[.+\\]/\n});\n\nlet ArgumentDefinition = createToken({\n  name: \"ArgumentDefinition\",\n  pattern: /\\<.+\\>\\:/\n});\n\nlet ArgumentReference = createToken({\n  name: \"ArgumentReference\",\n  pattern: /\\<.+\\>/\n});\n\nlet Newline = createToken({\n    name: \"Newline\",\n    pattern: /(\\n|\\r)/,\n    group: Lexer.SKIPPED\n});\n\nlet Spaces = createToken({\n    name: \"Spaces\",\n    pattern: /(' '|\\t)+/,\n    group: Lexer.SKIPPED\n});\nlet Freestyle = createToken({\n  name: \"Freestyle\",\n  pattern: /.+/\n});\nlet allTokens = [\n    Emptyline,\n    // Relation tokens must appear before Spaces, otherwise all indentation will always be consumed as spaces.\n    // Dedent must appear before Indent for handling zero spaces dedents.\n    Dedent,\n    Indent,\n    IncomingSupport,\n    IncomingAttack,\n    OutgoingSupport,\n    OutgoingAttack,\n    StatementDefinition,\n    StatementReference,\n    ArgumentDefinition,\n    ArgumentReference,\n    Newline,\n    Spaces,\n    Freestyle\n];\nlet customPatternLexer = new Lexer(allTokens);\n\nmodule.exports = {\n\n    // for testing purposes\n    Emptyline: Emptyline,\n    Indent: Indent,\n    Dedent: Dedent,\n    IncomingSupport: IncomingSupport,\n    IncomingAttack: IncomingAttack,\n    OutgoingSupport: OutgoingSupport,\n    OutgoingAttack: OutgoingAttack,\n    StatementReference: StatementReference,\n    StatementDefinition: StatementDefinition,\n    ArgumentReference: ArgumentReference,\n    ArgumentDefinition: ArgumentDefinition,\n    Freestyle: Freestyle,\n    Spaces: Spaces,\n\n    logTokens: function(lexResult){\n      for(let token of lexResult.tokens){\n        console.log(token.constructor.name+\" \"+token.image);\n      }\n    },\n\n    tokens: allTokens,\n\n    tokenize: function(text) {\n\n        // have to reset the indent stack between processing of different text inputs\n        indentStack = [0];\n\n        let lexResult = customPatternLexer.tokenize(text);\n\n        emitRemainingDedentTokens(lexResult.tokens);\n\n        if (lexResult.errors.length > 0) {\n            throw new Error(\"sad sad panda lexing errors detected\");\n        }\n        return lexResult;\n    }\n};\n"]}