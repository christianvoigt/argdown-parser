{"version":3,"sources":["../../src/ArgdownLexer.js"],"names":["chevrotain","_","createToken","ArgdownLexer","indentStack","rangesStack","matchedTokens","matchedTokensIsEmpty","isEmpty","last","currentLine","getEndLine","tokenMatcher","Emptyline","length","lastToken","tokens","lastOffset","getEndOffset","lastLine","lastColumn","getEndColumn","push","Dedent","pop","groups","indentStr","currIndentLevel","lastIndentLevel","image","offset","line","getCurrentLine","nl","column","indentToken","Indent","dedentToken","$","matchRelation","text","pattern","startsWithNewline","exec","match","emitIndentOrDedent","matchIncomingSupport","partialRight","matchIncomingAttack","matchOutgoingSupport","matchOutgoingAttack","IncomingSupport","name","IncomingAttack","OutgoingSupport","OutgoingAttack","inferenceStartPattern","matchInferenceStart","InferenceStart","push_mode","Colon","ListDelimiter","MetadataStatementEnd","MetadataStart","MetadataEnd","InferenceEnd","pop_mode","matchListItem","afterEmptyline","orderedListItemPattern","matchOrderedListItem","OrderedListItem","unorderedListItemPattern","matchUnorderedListItem","UnorderedListItem","argumentStatementStartPattern","matchArgumentStatementStart","emitRemainingDedentTokens","ArgumentStatementStart","emptylinePattern","matchEmptyline","Lexer","NA","StatementDefinition","StatementReference","StatementMention","ArgumentDefinition","ArgumentReference","ArgumentMention","headingPattern","matchHeadingStart","HeadingStart","matchBoldOrItalicStart","rangeType","matchBoldOrItalicEnd","lastRange","skipped","SKIPPED","lastSkipped","lastMatched","matchAsteriskBoldStart","matchAsteriskBoldEnd","matchUnderscoreBoldStart","matchUnderscoreBoldEnd","matchAsteriskItalicStart","matchAsteriskItalicEnd","matchUnderscoreItalicStart","matchUnderscoreItalicEnd","AsteriskBoldStart","AsteriskBoldEnd","UnderscoreBoldStart","UnderscoreBoldEnd","AsteriskItalicStart","AsteriskItalicEnd","UnderscoreItalicStart","UnderscoreItalicEnd","Comment","group","Link","Newline","Spaces","Freestyle","UnusedControlChar","lexerConfig","modes","defaultMode","_lexer","token","console","log","constructor","init","lexResult","tokenize","errors","Error","module","exports"],"mappings":"AAAA;;;;AAEA;;IAAYA,U;;AACZ;;IAAYC,C;;;;;;AAEZ,IAAMC,cAAcF,WAAWE,WAA/B;;IAEMC,Y;;;+BACK;AACH;AACA,iBAAKC,WAAL,GAAmB,CAAC,CAAD,CAAnB;AACA;AACA,iBAAKC,WAAL,GAAmB,EAAnB;AACH;;;uCACcC,a,EAAe;AAC1B,gBAAMC,uBAAuBN,EAAEO,OAAF,CAAUF,aAAV,CAA7B;AACA,gBAAIC,oBAAJ,EACI,OAAO,CAAP;;AAEJ,gBAAIE,OAAOR,EAAEQ,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAII,cAAcV,WAAWW,UAAX,CAAsBF,IAAtB,CAAlB;AACA,gBAAIT,WAAWY,YAAX,CAAwBH,IAAxB,EAA8B,KAAKI,SAAnC,CAAJ,EACIH;AACJ,mBAAOA,WAAP;AACH;;;kDAEyBJ,a,EAAe;AACrC,gBAAI,KAAKF,WAAL,CAAiBU,MAAjB,IAA2B,CAA/B,EACI;AACJ,gBAAMC,YAAYd,EAAEQ,IAAF,CAAOH,cAAcU,MAArB,CAAlB;AACA,gBAAMC,aAAcF,SAAD,GAAcf,WAAWkB,YAAX,CAAwBH,SAAxB,CAAd,GAAmD,CAAtE;AACA,gBAAMI,WAAYJ,SAAD,GAAcf,WAAWW,UAAX,CAAsBI,SAAtB,CAAd,GAAiD,CAAlE;AACA,gBAAMK,aAAcL,SAAD,GAAcf,WAAWqB,YAAX,CAAwBN,SAAxB,CAAd,GAAmD,CAAtE;;AAEA;AACA,mBAAO,KAAKX,WAAL,CAAiBU,MAAjB,GAA0B,CAAjC,EAAoC;AAChCR,8BAAcgB,IAAd,CAAmB,IAAI,KAAKC,MAAT,CAAgB,EAAhB,EAAoBN,UAApB,EAAgCE,QAAhC,EAA0CC,UAA1C,CAAnB;AACA,qBAAKhB,WAAL,CAAiBoB,GAAjB;AACH;AACJ;;;2CAEkBlB,a,EAAemB,M,EAAQC,S,EAAW;AACjD,gBAAMC,kBAAkBD,UAAUZ,MAAlC;AACA,gBAAMc,kBAAkB3B,EAAEQ,IAAF,CAAO,KAAKL,WAAZ,CAAxB;AACA,gBAAMyB,QAAQ,EAAd;AACA,gBAAMpB,OAAOR,EAAEQ,IAAF,CAAOH,aAAP,CAAb;AACA,gBAAMwB,SAAUrB,IAAD,GAAST,WAAWkB,YAAX,CAAwBT,IAAxB,IAAgC,CAAzC,GAA6C,CAA5D;AACA,gBAAMsB,OAAO,KAAKC,cAAL,CAAoB1B,aAApB,EAAmCmB,OAAOQ,EAA1C,CAAb;AACA,gBAAMC,SAAUzB,IAAD,GAAST,WAAWqB,YAAX,CAAwBZ,IAAxB,IAAgC,CAAzC,GAA6C,CAA5D;AACA,gBAAIkB,kBAAkBC,eAAtB,EAAuC;AACnC,qBAAKxB,WAAL,CAAiBkB,IAAjB,CAAsBK,eAAtB;AACA,oBAAIQ,cAAc,IAAI,KAAKC,MAAT,CAAgBP,KAAhB,EAAuBC,MAAvB,EAA+BC,IAA/B,EAAqCG,MAArC,CAAlB;AACA5B,8BAAcgB,IAAd,CAAmBa,WAAnB;AACH,aAJD,MAIO,IAAIR,kBAAkBC,eAAtB,EAAuC;AAC1C,uBAAO,KAAKxB,WAAL,CAAiBU,MAAjB,GAA0B,CAA1B,IAA+Ba,kBAAkB1B,EAAEQ,IAAF,CAAO,KAAKL,WAAZ,CAAxD,EAAkF;AAC9E,yBAAKA,WAAL,CAAiBoB,GAAjB;AACA,wBAAIa,cAAc,IAAI,KAAKd,MAAT,CAAgBM,KAAhB,EAAuBC,MAAvB,EAA+BC,IAA/B,EAAqCG,MAArC,CAAlB;AACA5B,kCAAcgB,IAAd,CAAmBe,WAAnB;AACH;AACJ;AACJ;;;AAED,4BAAc;AAAA;;AACV,YAAIC,IAAI,IAAR;AACA,iBAASC,aAAT,CAAuBC,IAAvB,EAA6BlC,aAA7B,EAA4CmB,MAA5C,EAAoDgB,OAApD,EAA6D;AACzD,gBAAIC,oBAAoB,kBAAkBC,IAAlB,CAAuBH,IAAvB,KAAgC,IAAxD;AACA,gBAAIvC,EAAEO,OAAF,CAAUF,aAAV,KAA4BoC,iBAAhC,EAAmD;AAC/C,oBAAIE,QAAQH,QAAQE,IAAR,CAAaH,IAAb,CAAZ;AACA,oBAAII,UAAU,IAAV,IAAkBA,MAAM9B,MAAN,IAAgB,CAAtC,EAAyC;AACrC,wBAAIY,YAAYkB,MAAM,CAAN,CAAhB;AACAN,sBAAEO,kBAAF,CAAqBvC,aAArB,EAAoCmB,MAApC,EAA4CC,SAA5C;AACA,2BAAOkB,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;AACD;AACA,YAAIE,uBAAuB7C,EAAE8C,YAAF,CAAeR,aAAf,EAA8B,iCAA9B,CAA3B;AACA,YAAIS,sBAAsB/C,EAAE8C,YAAF,CAAeR,aAAf,EAA8B,gCAA9B,CAA1B;AACA,YAAIU,uBAAuBhD,EAAE8C,YAAF,CAAeR,aAAf,EAA8B,kCAA9B,CAA3B;AACA,YAAIW,sBAAsBjD,EAAE8C,YAAF,CAAeR,aAAf,EAA8B,iCAA9B,CAA1B;;AAEAD,UAAEa,eAAF,GAAoBjD,YAAY;AAC5BkD,kBAAM,iBADsB;AAE5BX,qBAASK;AAFmB,SAAZ,CAApB;;AAKAR,UAAEe,cAAF,GAAmBnD,YAAY;AAC3BkD,kBAAM,gBADqB;AAE3BX,qBAASO;AAFkB,SAAZ,CAAnB;;AAKAV,UAAEgB,eAAF,GAAoBpD,YAAY;AAC5BkD,kBAAM,iBADsB;AAE5BX,qBAASQ;AAFmB,SAAZ,CAApB;;AAKAX,UAAEiB,cAAF,GAAmBrD,YAAY;AAC3BkD,kBAAM,gBADqB;AAE3BX,qBAASS;AAFkB,SAAZ,CAAnB;;AAKA,YAAMM,wBAAwB,4BAA9B;;AAEA,iBAASC,mBAAT,CAA6BjB,IAA7B,EAAmClC,aAAnC,EAAkD;AAC9C,gBAAIoC,oBAAoB,gBAAgBC,IAAhB,CAAqBH,IAArB,KAA8B,IAAtD;AACA,gBAAIvC,EAAEO,OAAF,CAAUF,aAAV,KAA4BoC,iBAAhC,EAAmD;AAC/C,uBAAOc,sBAAsBb,IAAtB,CAA2BH,IAA3B,CAAP;AACH;AACD,mBAAO,IAAP;AACH;AACDF,UAAEoB,cAAF,GAAmBxD,YAAY;AAC3BkD,kBAAM,gBADqB;AAE3BX,qBAASgB,mBAFkB;AAG3BE,uBAAW;AAHgB,SAAZ,CAAnB;;AAMArB,UAAEsB,KAAF,GAAU1D,YAAY;AAClBkD,kBAAM,OADY;AAElBX,qBAAS;AAFS,SAAZ,CAAV;AAIAH,UAAEuB,aAAF,GAAkB3D,YAAY;AAC1BkD,kBAAM,eADoB;AAE1BX,qBAAS;AAFiB,SAAZ,CAAlB;AAIAH,UAAEwB,oBAAF,GAAyB5D,YAAY;AACjCkD,kBAAM,sBAD2B;AAEjCX,qBAAS;AAFwB,SAAZ,CAAzB;AAIAH,UAAEyB,aAAF,GAAkB7D,YAAY;AAC1BkD,kBAAM,eADoB;AAE1BX,qBAAS;AAFiB,SAAZ,CAAlB;AAIAH,UAAE0B,WAAF,GAAgB9D,YAAY;AACxBkD,kBAAM,aADkB;AAExBX,qBAAS;AAFe,SAAZ,CAAhB;;AAKAH,UAAE2B,YAAF,GAAiB/D,YAAY;AACzBkD,kBAAM,cADmB;AAEzBX,qBAAS,OAFgB;AAGzByB,sBAAU;AAHe,SAAZ,CAAjB;;AAMA,iBAASC,aAAT,CAAuB3B,IAAvB,EAA6BlC,aAA7B,EAA4CmB,MAA5C,EAAoDgB,OAApD,EAA6D;AACzD,gBAAIC,oBAAoB,gBAAgBC,IAAhB,CAAqBH,IAArB,KAA8B,IAAtD;AACA,gBAAI4B,iBAAiBnE,EAAEQ,IAAF,CAAOH,aAAP,aAAiCgC,EAAEzB,SAAxD;AACA,gBAAIZ,EAAEO,OAAF,CAAUF,aAAV,KAA4B8D,cAA5B,IAA8C1B,iBAAlD,EAAqE;AACjE,oBAAIE,QAAQH,QAAQE,IAAR,CAAaH,IAAb,CAAZ;AACA,oBAAII,UAAU,IAAd,EAAoB;AAChB,wBAAIlB,YAAYkB,MAAM,CAAN,CAAhB;AACAN,sBAAEO,kBAAF,CAAqBvC,aAArB,EAAoCmB,MAApC,EAA4CC,SAA5C;AACA,2BAAOkB,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;;AAED,YAAMyB,yBAAyB,uCAA/B;AACA,YAAIC,uBAAuBrE,EAAE8C,YAAF,CAAeoB,aAAf,EAA8BE,sBAA9B,CAA3B;;AAEA/B,UAAEiC,eAAF,GAAoBrE,YAAY;AAC5BkD,kBAAM,iBADsB;AAE5BX,qBAAS6B;AAFmB,SAAZ,CAApB;AAIA;AACA,YAAME,2BAA2B,oCAAjC,CAxGU,CAwG6D;AACvE,YAAIC,yBAAyBxE,EAAE8C,YAAF,CAAeoB,aAAf,EAA8BK,wBAA9B,CAA7B;;AAEAlC,UAAEoC,iBAAF,GAAsBxE,YAAY;AAC9BkD,kBAAM,mBADwB;AAE9BX,qBAASgC;AAFqB,SAAZ,CAAtB;;AAKA,YAAME,gCAAgC,iCAAtC;;AAEA,iBAASC,2BAAT,CAAqCpC,IAArC,EAA2ClC,aAA3C,EAA0D;AACtD,gBAAIoC,oBAAoB,kBAAkBC,IAAlB,CAAuBH,IAAvB,KAAgC,IAAxD;AACA,gBAAI4B,iBAAiBnE,EAAEQ,IAAF,CAAOH,aAAP,aAAiCgC,EAAEzB,SAAxD;AACA,gBAAIZ,EAAEO,OAAF,CAAUF,aAAV,KAA4B8D,cAA5B,IAA8C1B,iBAAlD,EAAqE;AACjE,oBAAIE,QAAQ+B,8BAA8BhC,IAA9B,CAAmCH,IAAnC,CAAZ;AACA,oBAAGI,KAAH,EAAS;AACPN,sBAAEuC,yBAAF,CAA4BvE,aAA5B;AACA,2BAAOsC,KAAP;AACD;AACJ;AACD,mBAAO,IAAP;AACH;;AAEDN,UAAEwC,sBAAF,GAA2B5E,YAAY;AACnCkD,kBAAM,wBAD6B;AAEnCX,qBAASmC;AAF0B,SAAZ,CAA3B;;AAMA,YAAMG,mBAAmB,uBAAzB,CArIU,CAqIwC;AAClD,iBAASC,cAAT,CAAwBxC,IAAxB,EAA8BlC,aAA9B,EAA6C;AACzC,gBAAIG,OAAOR,EAAEQ,IAAF,CAAOH,aAAP,CAAX;AACA;AACA,gBAAIG,gBAAgB6B,EAAEzB,SAAtB,EACI,OAAO,IAAP;AACJ,gBAAI+B,QAAQmC,iBAAiBpC,IAAjB,CAAsBH,IAAtB,CAAZ;AACA,gBAAII,UAAU,IAAd,EAAoB;AAChBN,kBAAEuC,yBAAF,CAA4BvE,aAA5B;AACA;AACA,uBAAOsC,KAAP;AACH;AACD,mBAAO,IAAP;AACH;AACDN,UAAEzB,SAAF,GAAcX,YAAY;AACtBkD,kBAAM,WADgB;AAEtBX,qBAASuC;AAFa,SAAZ,CAAd;;AAKA;AACA1C,UAAEF,MAAF,GAAWlC,YAAY;AACnBkD,kBAAM,QADa;AAEnBX,qBAASzC,WAAWiF,KAAX,CAAiBC;AAFP,SAAZ,CAAX;AAIA5C,UAAEf,MAAF,GAAWrB,YAAY;AACnBkD,kBAAM,QADa;AAEnBX,qBAASzC,WAAWiF,KAAX,CAAiBC;AAFP,SAAZ,CAAX;;AAKA5C,UAAE6C,mBAAF,GAAwBjF,YAAY;AAChCkD,kBAAM,qBAD0B;AAEhCX,qBAAS;AAFuB,SAAZ,CAAxB;;AAKAH,UAAE8C,kBAAF,GAAuBlF,YAAY;AAC/BkD,kBAAM,oBADyB;AAE/BX,qBAAS;AAFsB,SAAZ,CAAvB;AAIAH,UAAE+C,gBAAF,GAAqBnF,YAAY;AAC/BkD,kBAAM,kBADyB;AAE/BX,qBAAS;AAFsB,SAAZ,CAArB;;AAKAH,UAAEgD,kBAAF,GAAuBpF,YAAY;AAC/BkD,kBAAM,oBADyB;AAE/BX,qBAAS;AAFsB,SAAZ,CAAvB;;AAKAH,UAAEiD,iBAAF,GAAsBrF,YAAY;AAC9BkD,kBAAM,mBADwB;AAE9BX,qBAAS;AAFqB,SAAZ,CAAtB;;AAKAH,UAAEkD,eAAF,GAAoBtF,YAAY;AAC9BkD,kBAAM,iBADwB;AAE9BX,qBAAS;AAFqB,SAAZ,CAApB;;AAKA,YAAMgD,iBAAiB,OAAvB;;AAEA,iBAASC,iBAAT,CAA2BlD,IAA3B,EAAiClC,aAAjC,EAAgD;AAC5C,gBAAI8D,iBAAiBnE,EAAEQ,IAAF,CAAOH,aAAP,aAAiCgC,EAAEzB,SAAxD;;AAEA,gBAAIZ,EAAEO,OAAF,CAAUF,aAAV,KAA4B8D,cAAhC,EAAgD;AAC5C,uBAAOqB,eAAe9C,IAAf,CAAoBH,IAApB,CAAP;AACH;AACD,mBAAO,IAAP;AAEH;;AAEDF,UAAEqD,YAAF,GAAiBzF,YAAY;AACzBkD,kBAAM,cADmB;AAEzBX,qBAASiD;AAFgB,SAAZ,CAAjB;;AAKA;AACA,iBAASE,sBAAT,CAAgCpD,IAAhC,EAAsClC,aAAtC,EAAqDmB,MAArD,EAA6DgB,OAA7D,EAAsEoD,SAAtE,EAAiF;AAC7E,gBAAIjD,QAAQH,QAAQE,IAAR,CAAaH,IAAb,CAAZ;AACA,gBAAII,SAAS,IAAb,EAAmB;AACfN,kBAAEjC,WAAF,CAAciB,IAAd,CAAmBuE,SAAnB;AACH;AACD,mBAAOjD,KAAP;AACH;;AAED,iBAASkD,oBAAT,CAA8BtD,IAA9B,EAAoClC,aAApC,EAAmDmB,MAAnD,EAA2DgB,OAA3D,EAAoEoD,SAApE,EAA+E;AAC3E,gBAAIE,YAAY9F,EAAEQ,IAAF,CAAO6B,EAAEjC,WAAT,CAAhB;AACA,gBAAI0F,aAAaF,SAAjB,EACI,OAAO,IAAP;AACJ;AACA,gBAAIG,UAAUvE,OAAOzB,WAAWiF,KAAX,CAAiBgB,OAAxB,CAAd;AACA,gBAAIC,cAAcjG,EAAEQ,IAAF,CAAOuF,OAAP,CAAlB;AACA,gBAAIG,cAAclG,EAAEQ,IAAF,CAAOH,aAAP,CAAlB;AACA,gBAAI,CAAC6F,WAAD,IACCD,eAAelG,WAAWkB,YAAX,CAAwBgF,WAAxB,IAAuClG,WAAWkB,YAAX,CAAwBiF,WAAxB,CAD3D,EACkG;AAC9F,uBAAO,IAAP;AACH;AACD,gBAAIvD,QAAQH,QAAQE,IAAR,CAAaH,IAAb,CAAZ;;AAEA,gBAAII,SAAS,IAAb,EAAmB;AACfN,kBAAEjC,WAAF,CAAcmB,GAAd;AACH;;AAED,mBAAOoB,KAAP;AACH;AACD,YAAIwD,yBAAyBnG,EAAE8C,YAAF,CAAe6C,sBAAf,EAAuC,aAAvC,EAAsD,cAAtD,CAA7B;AACA,YAAIS,uBAAuBpG,EAAE8C,YAAF,CAAe+C,oBAAf,EAAqC,8CAArC,EAAqF,cAArF,CAA3B;;AAEA,YAAIQ,2BAA2BrG,EAAE8C,YAAF,CAAe6C,sBAAf,EAAuC,WAAvC,EAAoD,gBAApD,CAA/B;AACA,YAAIW,yBAAyBtG,EAAE8C,YAAF,CAAe+C,oBAAf,EAAqC,4CAArC,EAAmF,gBAAnF,CAA7B;;AAEA,YAAIU,2BAA2BvG,EAAE8C,YAAF,CAAe6C,sBAAf,EAAuC,WAAvC,EAAoD,gBAApD,CAA/B;AACA,YAAIa,yBAAyBxG,EAAE8C,YAAF,CAAe+C,oBAAf,EAAqC,4CAArC,EAAmF,gBAAnF,CAA7B;;AAEA,YAAIY,6BAA6BzG,EAAE8C,YAAF,CAAe6C,sBAAf,EAAuC,WAAvC,EAAoD,kBAApD,CAAjC;AACA,YAAIe,2BAA2B1G,EAAE8C,YAAF,CAAe+C,oBAAf,EAAqC,4CAArC,EAAmF,kBAAnF,CAA/B;;AAGAxD,UAAEsE,iBAAF,GAAsB1G,YAAY;AAC9BkD,kBAAM,mBADwB;AAE9BX,qBAAS2D;AAFqB,SAAZ,CAAtB;;AAKA9D,UAAEuE,eAAF,GAAoB3G,YAAY;AAC5BkD,kBAAM,iBADsB;AAE5BX,qBAAS4D;AAFmB,SAAZ,CAApB;AAIA/D,UAAEwE,mBAAF,GAAwB5G,YAAY;AAChCkD,kBAAM,qBAD0B;AAEhCX,qBAAS6D;AAFuB,SAAZ,CAAxB;;AAKAhE,UAAEyE,iBAAF,GAAsB7G,YAAY;AAC9BkD,kBAAM,mBADwB;AAE9BX,qBAAS8D;AAFqB,SAAZ,CAAtB;;AAKAjE,UAAE0E,mBAAF,GAAwB9G,YAAY;AAChCkD,kBAAM,qBAD0B;AAEhCX,qBAAS+D;AAFuB,SAAZ,CAAxB;;AAKAlE,UAAE2E,iBAAF,GAAsB/G,YAAY;AAC9BkD,kBAAM,mBADwB;AAE9BX,qBAASgE;AAFqB,SAAZ,CAAtB;;AAKAnE,UAAE4E,qBAAF,GAA0BhH,YAAY;AAClCkD,kBAAM,uBAD4B;AAElCX,qBAASiE;AAFyB,SAAZ,CAA1B;;AAKApE,UAAE6E,mBAAF,GAAwBjH,YAAY;AAChCkD,kBAAM,qBAD0B;AAEhCX,qBAASkE;AAFuB,SAAZ,CAAxB;;AAKArE,UAAE8E,OAAF,GAAYlH,YAAY;AACpBkD,kBAAM,SADc;AAEpBX,qBAAS,sBAFW;AAGpB4E,mBAAOrH,WAAWiF,KAAX,CAAiBgB;AAHJ,SAAZ,CAAZ;;AAMA3D,UAAEgF,IAAF,GAASpH,YAAY;AACjBkD,kBAAM,MADW;AAEjBX,qBAAS;AAFQ,SAAZ,CAAT;;AAKAH,UAAEiF,OAAF,GAAYrH,YAAY;AACpBkD,kBAAM,SADc;AAEpBX,qBAAS,gBAFW;AAGpB4E,mBAAOrH,WAAWiF,KAAX,CAAiBgB;AAHJ,SAAZ,CAAZ;;AAMA3D,UAAEkF,MAAF,GAAWtH,YAAY;AACnBkD,kBAAM,QADa;AAEnBX,qBAAS,SAFU;AAGnB4E,mBAAOrH,WAAWiF,KAAX,CAAiBgB;AAHL,SAAZ,CAAX;;AAMA;AACA3D,UAAEmF,SAAF,GAAcvH,YAAY;AACtBkD,kBAAM,WADgB;AAEtBX,qBAAS;AAFa,SAAZ,CAAd;;AAKA;AACA;AACA;AACA;AACAH,UAAEoF,iBAAF,GAAsBxH,YAAY;AAC9BkD,kBAAM,mBADwB;AAE9BX,qBAAS;AAFqB,SAAZ,CAAtB;;AAKAH,UAAEtB,MAAF,GAAW,CACPsB,EAAE8E,OADK,EAEP9E,EAAEzB,SAFK,EAGPyB,EAAEf,MAHK,EAIPe,EAAEF,MAJK,EAKPE,EAAEoB,cALK,EAKW;AAClBpB,UAAE2B,YANK,EAOP3B,EAAEwB,oBAPK,EAQPxB,EAAEyB,aARK,EASPzB,EAAE0B,WATK,EAUP1B,EAAEuB,aAVK,EAWPvB,EAAEsB,KAXK,EAYPtB,EAAEa,eAZK,EAaPb,EAAEe,cAbK,EAcPf,EAAEgB,eAdK,EAePhB,EAAEiB,cAfK,EAgBPjB,EAAEqD,YAhBK,EAiBPrD,EAAEwC,sBAjBK,EAkBPxC,EAAEiC,eAlBK,EAmBPjC,EAAEoC,iBAnBK;AAoBP;AACApC,UAAEuE,eArBK,EAqBY;AACnBvE,UAAEyE,iBAtBK,EAsBc;AACrBzE,UAAE2E,iBAvBK,EAwBP3E,EAAE6E,mBAxBK;AAyBP;AACA7E,UAAEsE,iBA1BK,EA0Bc;AACrBtE,UAAEwE,mBA3BK,EA2BgB;AACvBxE,UAAE0E,mBA5BK,EA6BP1E,EAAE4E,qBA7BK,EA8BP5E,EAAEgF,IA9BK,EA8BC;AACRhF,UAAE+C,gBA/BK,EAgCP/C,EAAE6C,mBAhCK,EAiCP7C,EAAE8C,kBAjCK,EAkCP9C,EAAEkD,eAlCK,EAmCPlD,EAAEgD,kBAnCK,EAoCPhD,EAAEiD,iBApCK,EAqCPjD,EAAEiF,OArCK,EAsCPjF,EAAEkF,MAtCK,EAuCPlF,EAAEmF,SAvCK,EAwCPnF,EAAEoF,iBAxCK,CAAX;;AA2CA,YAAIC,cAAc;;AAEdC,mBAAO;AACH,gCAAgB,CACZtF,EAAE8E,OADU,EAEZ9E,EAAEzB,SAFU;AAGZ;AACA;AACAyB,kBAAEf,MALU,EAMZe,EAAEF,MANU,EAOZE,EAAEoB,cAPU,EAOM;AAClBpB,kBAAEa,eARU,EASZb,EAAEe,cATU,EAUZf,EAAEgB,eAVU,EAWZhB,EAAEiB,cAXU,EAYZjB,EAAEqD,YAZU,EAaZrD,EAAEwC,sBAbU,EAcZxC,EAAEiC,eAdU,EAeZjC,EAAEoC,iBAfU;AAgBZ;AACApC,kBAAEuE,eAjBU,EAiBO;AACnBvE,kBAAEyE,iBAlBU,EAkBS;AACrBzE,kBAAE2E,iBAnBU,EAoBZ3E,EAAE6E,mBApBU;AAqBZ;AACA7E,kBAAEsE,iBAtBU,EAsBS;AACrBtE,kBAAEwE,mBAvBU,EAuBW;AACvBxE,kBAAE0E,mBAxBU,EAyBZ1E,EAAE4E,qBAzBU,EA0BZ5E,EAAEgF,IA1BU,EA0BJ;AACRhF,kBAAE6C,mBA3BU,EA4BZ7C,EAAE8C,kBA5BU,EA6BZ9C,EAAE+C,gBA7BU,EA8BZ/C,EAAEgD,kBA9BU,EA+BZhD,EAAEiD,iBA/BU,EAgCZjD,EAAEkD,eAhCU,EAiCZlD,EAAEiF,OAjCU,EAkCZjF,EAAEkF,MAlCU,EAmCZlF,EAAEmF,SAnCU,EAoCZnF,EAAEoF,iBApCU,CADb;AAuCH,kCAAkB,CACdpF,EAAE8E,OADY,EAEd9E,EAAE2B,YAFY,EAGd3B,EAAEyB,aAHY,EAIdzB,EAAE0B,WAJY,EAKd1B,EAAEwB,oBALY,EAMdxB,EAAEuB,aANY,EAOdvB,EAAEsB,KAPY,EAQdtB,EAAEiF,OARY,EASdjF,EAAEkF,MATY,EAUdlF,EAAEmF,SAVY,EAWdnF,EAAEoF,iBAXY;AAvCf,aAFO;;AAwDdG,yBAAa;AAxDC,SAAlB;;AA2DA,aAAKC,MAAL,GAAc,IAAI9H,WAAWiF,KAAf,CAAqB0C,WAArB,CAAd;AAEH;;;;kCAES3G,M,EAAQ;AAAA;AAAA;AAAA;;AAAA;AACd,qCAAkBA,MAAlB,8HAA0B;AAAA,wBAAjB+G,KAAiB;;AACtBC,4BAAQC,GAAR,CAAYF,MAAMG,WAAN,CAAkB9E,IAAlB,GAAyB,GAAzB,GAA+B2E,MAAMlG,KAAjD;AACH;AAHa;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAIjB;;;iCACQW,I,EAAM;AACX,iBAAK2F,IAAL;;AAEA,gBAAIC,YAAY,KAAKN,MAAL,CAAYO,QAAZ,CAAqB7F,IAArB,CAAhB;;AAEA,iBAAKqC,yBAAL,CAA+BuD,UAAUpH,MAAzC;;AAEA,gBAAIoH,UAAUE,MAAV,CAAiBxH,MAAjB,GAA0B,CAA9B,EAAiC;AAC7B,sBAAM,IAAIyH,KAAJ,CAAU,sCAAV,CAAN;AACH;AACD,mBAAOH,SAAP;AACH;;;;;;AAILI,OAAOC,OAAP,GAAiB;AACbtI,kBAAc,IAAIA,YAAJ;AADD,CAAjB","file":"ArgdownLexer.js","sourcesContent":["'use strict';\n\nimport * as chevrotain from 'chevrotain';\nimport * as _ from 'lodash';\n\nconst createToken = chevrotain.createToken;\n\nclass ArgdownLexer {\n    init() {\n        // State required for matching the indentations\n        this.indentStack = [0];\n        // State require for matching bold and italic ranges in the right order\n        this.rangesStack = [];\n    }\n    getCurrentLine(matchedTokens) {\n        const matchedTokensIsEmpty = _.isEmpty(matchedTokens);\n        if (matchedTokensIsEmpty)\n            return 0;\n\n        let last = _.last(matchedTokens);\n        let currentLine = chevrotain.getEndLine(last);\n        if (chevrotain.tokenMatcher(last, this.Emptyline))\n            currentLine++;\n        return currentLine;\n    }\n\n    emitRemainingDedentTokens(matchedTokens) {\n        if (this.indentStack.length <= 1)\n            return;\n        const lastToken = _.last(matchedTokens.tokens);\n        const lastOffset = (lastToken) ? chevrotain.getEndOffset(lastToken) : 0;\n        const lastLine = (lastToken) ? chevrotain.getEndLine(lastToken) : 0;\n        const lastColumn = (lastToken) ? chevrotain.getEndColumn(lastToken) : 0;\n\n        //add remaining Dedents\n        while (this.indentStack.length > 1) {\n            matchedTokens.push(new this.Dedent(\"\", lastOffset, lastLine, lastColumn));\n            this.indentStack.pop();\n        }\n    }\n\n    emitIndentOrDedent(matchedTokens, groups, indentStr) {\n        const currIndentLevel = indentStr.length;\n        const lastIndentLevel = _.last(this.indentStack);\n        const image = \"\";\n        const last = _.last(matchedTokens);\n        const offset = (last) ? chevrotain.getEndOffset(last) + 1 : 0;\n        const line = this.getCurrentLine(matchedTokens, groups.nl);\n        const column = (last) ? chevrotain.getEndColumn(last) + 1 : 0;\n        if (currIndentLevel > lastIndentLevel) {\n            this.indentStack.push(currIndentLevel);\n            let indentToken = new this.Indent(image, offset, line, column);\n            matchedTokens.push(indentToken);\n        } else if (currIndentLevel < lastIndentLevel) {\n            while (this.indentStack.length > 1 && currIndentLevel < _.last(this.indentStack)) {\n                this.indentStack.pop();\n                let dedentToken = new this.Dedent(image, offset, line, column);\n                matchedTokens.push(dedentToken);\n            }\n        }\n    }\n\n    constructor() {\n        let $ = this;\n        function matchRelation(text, matchedTokens, groups, pattern) {\n            let startsWithNewline = /^(?:\\n\\r|\\n|\\r)/.exec(text) != null;\n            if (_.isEmpty(matchedTokens) || startsWithNewline) {\n                let match = pattern.exec(text);\n                if (match !== null && match.length == 3) {\n                    let indentStr = match[1];\n                    $.emitIndentOrDedent(matchedTokens, groups, indentStr);\n                    return match;\n                }\n            }\n            return null;\n        }\n        //relations start at BOF or after a newline, optionally followed by indentation (spaces or tabs)\n        let matchIncomingSupport = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(\\+>)/);\n        let matchIncomingAttack = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(->)/);\n        let matchOutgoingSupport = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(<?\\+)/);\n        let matchOutgoingAttack = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(<?-)/);\n\n        $.IncomingSupport = createToken({\n            name: \"IncomingSupport\",\n            pattern: matchIncomingSupport\n        });\n\n        $.IncomingAttack = createToken({\n            name: \"IncomingAttack\",\n            pattern: matchIncomingAttack\n        });\n\n        $.OutgoingSupport = createToken({\n            name: \"OutgoingSupport\",\n            pattern: matchOutgoingSupport\n        });\n\n        $.OutgoingAttack = createToken({\n            name: \"OutgoingAttack\",\n            pattern: matchOutgoingAttack\n        });\n\n        const inferenceStartPattern = /^[\\n\\r|\\n|\\r]?[' '\\t]*-{2}/;\n\n        function matchInferenceStart(text, matchedTokens) {\n            let startsWithNewline = /^[\\n\\r|\\n|\\r]/.exec(text) != null;\n            if (_.isEmpty(matchedTokens) || startsWithNewline) {\n                return inferenceStartPattern.exec(text);\n            }\n            return null;\n        }\n        $.InferenceStart = createToken({\n            name: \"InferenceStart\",\n            pattern: matchInferenceStart,\n            push_mode: \"inference_mode\"\n        });\n\n        $.Colon = createToken({\n            name: \"Colon\",\n            pattern: /:/\n        });\n        $.ListDelimiter = createToken({\n            name: \"ListDelimiter\",\n            pattern: /,/\n        });\n        $.MetadataStatementEnd = createToken({\n            name: \"MetadataStatementEnd\",\n            pattern: /;/\n        });\n        $.MetadataStart = createToken({\n            name: \"MetadataStart\",\n            pattern: /\\(/\n        });\n        $.MetadataEnd = createToken({\n            name: \"MetadataEnd\",\n            pattern: /\\)/\n        });\n\n        $.InferenceEnd = createToken({\n            name: \"InferenceEnd\",\n            pattern: /-{2,}/,\n            pop_mode: true\n        });\n\n        function matchListItem(text, matchedTokens, groups, pattern) {\n            let startsWithNewline = /^[\\n\\r|\\n|\\r]/.exec(text) != null;\n            let afterEmptyline = _.last(matchedTokens) instanceof $.Emptyline;\n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) {\n                let match = pattern.exec(text);\n                if (match !== null) {\n                    let indentStr = match[1];\n                    $.emitIndentOrDedent(matchedTokens, groups, indentStr);\n                    return match;\n                }\n            }\n            return null;\n        }\n\n        const orderedListItemPattern = /^(?:\\n\\r|\\n|\\r)?([' '\\t]+)\\d+\\.(?=\\s)/;\n        let matchOrderedListItem = _.partialRight(matchListItem, orderedListItemPattern);\n\n        $.OrderedListItem = createToken({\n            name: \"OrderedListItem\",\n            pattern: matchOrderedListItem\n        });\n        //whitespace + * + whitespace (to distinguish list items from bold and italic ranges)\n        const unorderedListItemPattern = /^(?:\\n\\r|\\n|\\r)?([' '\\t]+)\\*(?=\\s)/; //Newline +\n        let matchUnorderedListItem = _.partialRight(matchListItem, unorderedListItemPattern);\n\n        $.UnorderedListItem = createToken({\n            name: \"UnorderedListItem\",\n            pattern: matchUnorderedListItem\n        });\n\n        const argumentStatementStartPattern = /^(?:\\n\\r|\\n|\\r)?[' '\\t]*\\(\\d+\\)/;\n\n        function matchArgumentStatementStart(text, matchedTokens) {\n            let startsWithNewline = /^(?:\\n\\r|\\n|\\r)/.exec(text) != null;\n            let afterEmptyline = _.last(matchedTokens) instanceof $.Emptyline;\n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) {\n                let match = argumentStatementStartPattern.exec(text);\n                if(match){\n                  $.emitRemainingDedentTokens(matchedTokens);\n                  return match;\n                }\n            }\n            return null;\n        }\n\n        $.ArgumentStatementStart = createToken({\n            name: \"ArgumentStatementStart\",\n            pattern: matchArgumentStatementStart\n        });\n\n\n        const emptylinePattern = /^((?:\\n\\r|\\n|\\r){2,})/; //two or more linebreaks\n        function matchEmptyline(text, matchedTokens) {\n            let last = _.last(matchedTokens);\n            //ignore Emptylines after first one (relevant for Emptylines after ignored comments)\n            if (last instanceof $.Emptyline)\n                return null;\n            let match = emptylinePattern.exec(text);\n            if (match !== null) {\n                $.emitRemainingDedentTokens(matchedTokens);\n                //TODO: emitRemainingRanges (to be more resistant against unclosed bold and italic ranges)\n                return match;\n            }\n            return null;\n        }\n        $.Emptyline = createToken({\n            name: \"Emptyline\",\n            pattern: matchEmptyline\n        });\n\n        //Indent and Dedent are never matched with their own patterns, instead they get matched in the relations custom patterns\n        $.Indent = createToken({\n            name: \"Indent\",\n            pattern: chevrotain.Lexer.NA\n        });\n        $.Dedent = createToken({\n            name: \"Dedent\",\n            pattern: chevrotain.Lexer.NA\n        });\n\n        $.StatementDefinition = createToken({\n            name: \"StatementDefinition\",\n            pattern: /\\[.+?\\]\\:/\n        });\n\n        $.StatementReference = createToken({\n            name: \"StatementReference\",\n            pattern: /\\[.+?\\]/\n        });\n        $.StatementMention = createToken({\n          name: \"StatementMention\",\n          pattern: /\\@\\[.+?\\][ \\t]?/\n        });\n\n        $.ArgumentDefinition = createToken({\n            name: \"ArgumentDefinition\",\n            pattern: /\\<.+?\\>\\:/\n        });\n\n        $.ArgumentReference = createToken({\n            name: \"ArgumentReference\",\n            pattern: /\\<.+?\\>/\n        });\n\n        $.ArgumentMention = createToken({\n          name: \"ArgumentMention\",\n          pattern: /\\@\\<.+?\\>[ \\t]?/\n        });\n\n        const headingPattern = /^(#+)/;\n\n        function matchHeadingStart(text, matchedTokens) {\n            let afterEmptyline = _.last(matchedTokens) instanceof $.Emptyline;\n\n            if (_.isEmpty(matchedTokens) || afterEmptyline) {\n                return headingPattern.exec(text);\n            }\n            return null;\n\n        }\n\n        $.HeadingStart = createToken({\n            name: \"HeadingStart\",\n            pattern: matchHeadingStart\n        });\n\n        //BOLD and ITALIC ranges\n        function matchBoldOrItalicStart(text, matchedTokens, groups, pattern, rangeType) {\n            let match = pattern.exec(text);\n            if (match != null) {\n                $.rangesStack.push(rangeType);\n            }\n            return match;\n        }\n\n        function matchBoldOrItalicEnd(text, matchedTokens, groups, pattern, rangeType) {\n            let lastRange = _.last($.rangesStack);\n            if (lastRange != rangeType)\n                return null;\n            //first check if the last match was skipped Whitespace\n            let skipped = groups[chevrotain.Lexer.SKIPPED];\n            let lastSkipped = _.last(skipped);\n            let lastMatched = _.last(matchedTokens);\n            if (!lastMatched ||\n                (lastSkipped && chevrotain.getEndOffset(lastSkipped) > chevrotain.getEndOffset(lastMatched))) {\n                return null;\n            }\n            let match = pattern.exec(text);\n\n            if (match != null) {\n                $.rangesStack.pop();\n            }\n\n            return match;\n        }\n        let matchAsteriskBoldStart = _.partialRight(matchBoldOrItalicStart, /^\\*\\*(?!\\s)/, \"AsteriskBold\");\n        let matchAsteriskBoldEnd = _.partialRight(matchBoldOrItalicEnd, /^\\*\\*(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"AsteriskBold\");\n\n        let matchUnderscoreBoldStart = _.partialRight(matchBoldOrItalicStart, /^__(?!\\s)/, \"UnderscoreBold\");\n        let matchUnderscoreBoldEnd = _.partialRight(matchBoldOrItalicEnd, /^__(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"UnderscoreBold\");\n\n        let matchAsteriskItalicStart = _.partialRight(matchBoldOrItalicStart, /^\\*(?!\\s)/, \"AsteriskItalic\");\n        let matchAsteriskItalicEnd = _.partialRight(matchBoldOrItalicEnd, /^\\*(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"AsteriskItalic\");\n\n        let matchUnderscoreItalicStart = _.partialRight(matchBoldOrItalicStart, /^\\_(?!\\s)/, \"UnderscoreItalic\");\n        let matchUnderscoreItalicEnd = _.partialRight(matchBoldOrItalicEnd, /^\\_(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"UnderscoreItalic\");\n\n\n        $.AsteriskBoldStart = createToken({\n            name: \"AsteriskBoldStart\",\n            pattern: matchAsteriskBoldStart\n        });\n\n        $.AsteriskBoldEnd = createToken({\n            name: \"AsteriskBoldEnd\",\n            pattern: matchAsteriskBoldEnd\n        });\n        $.UnderscoreBoldStart = createToken({\n            name: \"UnderscoreBoldStart\",\n            pattern: matchUnderscoreBoldStart\n        });\n\n        $.UnderscoreBoldEnd = createToken({\n            name: \"UnderscoreBoldEnd\",\n            pattern: matchUnderscoreBoldEnd\n        });\n\n        $.AsteriskItalicStart = createToken({\n            name: \"AsteriskItalicStart\",\n            pattern: matchAsteriskItalicStart\n        });\n\n        $.AsteriskItalicEnd = createToken({\n            name: \"AsteriskItalicEnd\",\n            pattern: matchAsteriskItalicEnd\n        });\n\n        $.UnderscoreItalicStart = createToken({\n            name: \"UnderscoreItalicStart\",\n            pattern: matchUnderscoreItalicStart\n        });\n\n        $.UnderscoreItalicEnd = createToken({\n            name: \"UnderscoreItalicEnd\",\n            pattern: matchUnderscoreItalicEnd\n        });\n\n        $.Comment = createToken({\n            name: \"Comment\",\n            pattern: /<!--(?:.|\\n|\\r)*?-->/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n\n        $.Link = createToken({\n            name: \"Link\",\n            pattern: /\\[[^\\]]+?\\]\\([^\\)]+?\\)[ \\t]?/\n        });\n\n        $.Newline = createToken({\n            name: \"Newline\",\n            pattern: /(?:\\n\\r|\\n|\\r)/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n\n        $.Spaces = createToken({\n            name: \"Spaces\",\n            pattern: /( |\\t)+/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n\n        //The rest of the text that is free of any Argdown syntax\n        $.Freestyle = createToken({\n            name: \"Freestyle\",\n            pattern: /[^\\@\\*\\_\\[\\]\\,\\:\\;\\<\\/\\>\\-\\n\\r\\(\\)]+/\n        });\n\n        //Freestyle text needs to be \"cut up\" by these control characters so that the other rules get a chance to succeed.\n        //Otherwise, every line would simply be lexed as a single Freestyle token.\n        //If these chars are not consumed by other rules, they are lexed as \"useless\" UnusedControlChars. The parser then has to combine Freestyle and UnusedControlChar tokens back together to get \"normal text\" token sequences.\n        //Note that some \"meaningful\" characters (like +) are not listed here, as they are only meaningful after a linebreak and freestyle text already gets \"cut up\" by each line break.\n        $.UnusedControlChar = createToken({\n            name: \"UnusedControlChar\",\n            pattern: /[\\@\\*\\_\\[\\]\\,\\:\\;\\<\\/\\>\\-\\(\\)][ \\t]?/\n        });\n\n        $.tokens = [\n            $.Comment,\n            $.Emptyline,\n            $.Dedent,\n            $.Indent,\n            $.InferenceStart, //needs to be lexed before OutgoingAttack (- vs --)\n            $.InferenceEnd,\n            $.MetadataStatementEnd,\n            $.MetadataStart,\n            $.MetadataEnd,\n            $.ListDelimiter,\n            $.Colon,\n            $.IncomingSupport,\n            $.IncomingAttack,\n            $.OutgoingSupport,\n            $.OutgoingAttack,\n            $.HeadingStart,\n            $.ArgumentStatementStart,\n            $.OrderedListItem,\n            $.UnorderedListItem,\n            //The ends of Bold and italic ranges need to be lexed before the starts\n            $.AsteriskBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (** vs *)\n            $.UnderscoreBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (__ vs _)\n            $.AsteriskItalicEnd,\n            $.UnderscoreItalicEnd,\n            //The starts of Bold and italic ranges need to be lexed after the ends\n            $.AsteriskBoldStart, //BoldStart needs to be lexed before ItalicStart (** vs *)\n            $.UnderscoreBoldStart, //BoldStart needs to be lexed before ItalicStart (__ vs _)\n            $.AsteriskItalicStart,\n            $.UnderscoreItalicStart,\n            $.Link, //needs to be lexed before StatementReference\n            $.StatementMention,\n            $.StatementDefinition,\n            $.StatementReference,\n            $.ArgumentMention,\n            $.ArgumentDefinition,\n            $.ArgumentReference,\n            $.Newline,\n            $.Spaces,\n            $.Freestyle,\n            $.UnusedControlChar\n        ];\n\n        let lexerConfig = {\n\n            modes: {\n                \"default_mode\": [\n                    $.Comment,\n                    $.Emptyline,\n                    // Relation tokens must appear before Spaces, otherwise all indentation will always be consumed as spaces.\n                    // Dedent must appear before Indent for handling zero spaces dedents.\n                    $.Dedent,\n                    $.Indent,\n                    $.InferenceStart, //needs to be lexed before OutgoingAttack (- vs --)\n                    $.IncomingSupport,\n                    $.IncomingAttack,\n                    $.OutgoingSupport,\n                    $.OutgoingAttack,\n                    $.HeadingStart,\n                    $.ArgumentStatementStart,\n                    $.OrderedListItem,\n                    $.UnorderedListItem,\n                    //The ends of Bold and italic ranges need to be lexed before the starts\n                    $.AsteriskBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (** vs *)\n                    $.UnderscoreBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (__ vs _)\n                    $.AsteriskItalicEnd,\n                    $.UnderscoreItalicEnd,\n                    //The starts of Bold and italic ranges need to be lexed after the ends\n                    $.AsteriskBoldStart, //BoldStart needs to be lexed before ItalicStart (** vs *)\n                    $.UnderscoreBoldStart, //BoldStart needs to be lexed before ItalicStart (__ vs _)\n                    $.AsteriskItalicStart,\n                    $.UnderscoreItalicStart,\n                    $.Link, //needs to be lexed before StatementReference\n                    $.StatementDefinition,\n                    $.StatementReference,\n                    $.StatementMention,\n                    $.ArgumentDefinition,\n                    $.ArgumentReference,\n                    $.ArgumentMention,\n                    $.Newline,\n                    $.Spaces,\n                    $.Freestyle,\n                    $.UnusedControlChar\n                ],\n                \"inference_mode\": [\n                    $.Comment,\n                    $.InferenceEnd,\n                    $.MetadataStart,\n                    $.MetadataEnd,\n                    $.MetadataStatementEnd,\n                    $.ListDelimiter,\n                    $.Colon,\n                    $.Newline,\n                    $.Spaces,\n                    $.Freestyle,\n                    $.UnusedControlChar\n                ]\n            },\n\n            defaultMode: \"default_mode\"\n        };\n\n        this._lexer = new chevrotain.Lexer(lexerConfig);\n\n    }\n\n    logTokens(tokens) {\n        for (let token of tokens) {\n            console.log(token.constructor.name + \" \" + token.image);\n        }\n    }\n    tokenize(text) {\n        this.init();\n\n        let lexResult = this._lexer.tokenize(text);\n\n        this.emitRemainingDedentTokens(lexResult.tokens);\n\n        if (lexResult.errors.length > 0) {\n            throw new Error(\"sad sad panda lexing errors detected\");\n        }\n        return lexResult;\n    }\n\n}\n\nmodule.exports = {\n    ArgdownLexer: new ArgdownLexer()\n};\n"]}