{"version":3,"sources":["../../src/ArgdownLexer.js"],"names":["chevrotain","_","createToken","createTokenInstance","tokenMatcher","getTokenConstructor","ArgdownLexer","indentStack","rangesStack","matchedTokens","matchedTokensIsEmpty","isEmpty","last","currentLine","endLine","Emptyline","length","lastToken","tokens","startOffset","endOffset","startLine","startColumn","endColumn","push","Dedent","pop","groups","indentStr","currIndentLevel","lastIndentLevel","image","getCurrentLine","nl","indentToken","Indent","dedentToken","$","matchRelation","text","offset","pattern","remainingText","substr","startsWithNewline","exec","match","emitIndentOrDedent","matchIncomingSupport","partialRight","matchIncomingAttack","matchOutgoingSupport","matchOutgoingAttack","matchContradiction","IncomingSupport","name","IncomingAttack","OutgoingSupport","OutgoingAttack","Contradiction","inferenceStartPattern","matchInferenceStart","emitRemainingDedentTokens","InferenceStart","push_mode","Colon","ListDelimiter","MetadataStatementEnd","MetadataStart","MetadataEnd","InferenceEnd","pop_mode","matchListItem","afterEmptyline","orderedListItemPattern","matchOrderedListItem","OrderedListItem","unorderedListItemPattern","matchUnorderedListItem","UnorderedListItem","argumentStatementStartPattern","matchArgumentStatementStart","ArgumentStatementStart","emptylinePattern","matchEmptyline","Lexer","NA","StatementDefinition","StatementReference","StatementMention","ArgumentDefinition","ArgumentReference","ArgumentMention","headingPattern","matchHeadingStart","HeadingStart","matchBoldOrItalicStart","rangeType","matchBoldOrItalicEnd","lastRange","skipped","SKIPPED","lastSkipped","lastMatched","getEndOffset","matchAsteriskBoldStart","matchAsteriskBoldEnd","matchUnderscoreBoldStart","matchUnderscoreBoldEnd","matchAsteriskItalicStart","matchAsteriskItalicEnd","matchUnderscoreItalicStart","matchUnderscoreItalicEnd","AsteriskBoldStart","AsteriskBoldEnd","UnderscoreBoldStart","UnderscoreBoldEnd","AsteriskItalicStart","AsteriskItalicEnd","UnderscoreItalicStart","UnderscoreItalicEnd","Comment","group","Link","Newline","Spaces","Freestyle","UnusedControlChar","lexerConfig","modes","defaultMode","_lexer","str","token","tokenName","init","lexResult","tokenize","errors","Error","module","exports"],"mappings":"AAAA;;;;AAEA;;IAAYA,U;;AACZ;;IAAYC,C;;;;;;AAEZ,IAAMC,cAAcF,WAAWE,WAA/B;AACA,IAAMC,sBAAsBH,WAAWG,mBAAvC;AACA,IAAMC,eAAeJ,WAAWI,YAAhC;AACA,IAAMC,sBAAsBL,WAAWK,mBAAvC;;IAEMC,Y;;;+BACK;AACH;AACA,iBAAKC,WAAL,GAAmB,CAAC,CAAD,CAAnB;AACA;AACA,iBAAKC,WAAL,GAAmB,EAAnB;AACH;;;uCACcC,a,EAAe;AAC1B,gBAAMC,uBAAuBT,EAAEU,OAAF,CAAUF,aAAV,CAA7B;AACA,gBAAIC,oBAAJ,EACI,OAAO,CAAP;;AAEJ,gBAAIE,OAAOX,EAAEW,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAII,cAAeD,IAAD,GAAQA,KAAKE,OAAb,GAAuB,CAAzC;AACA,gBAAIF,QAAQZ,WAAWI,YAAX,CAAwBQ,IAAxB,EAA8B,KAAKG,SAAnC,CAAZ,EACIF;AACJ,mBAAOA,WAAP;AACH;;;kDAEyBJ,a,EAAe;AACrC,gBAAI,KAAKF,WAAL,CAAiBS,MAAjB,IAA2B,CAA/B,EACI;AACJ,gBAAMC,YAAYhB,EAAEW,IAAF,CAAOH,cAAcS,MAArB,CAAlB;AACA,gBAAMC,cAAeF,SAAD,GAAcA,UAAUG,SAAxB,GAAoC,CAAxD;AACA,gBAAMA,YAAYD,WAAlB;AACA,gBAAME,YAAaJ,SAAD,GAAcA,UAAUH,OAAxB,GAAkC,CAApD;AACA,gBAAMA,UAAUO,SAAhB;AACA,gBAAMC,cAAeL,SAAD,GAAcA,UAAUM,SAAxB,GAAoC,CAAxD;AACA,gBAAMA,YAAYD,WAAlB;;AAEA;AACA,mBAAO,KAAKf,WAAL,CAAiBS,MAAjB,GAA0B,CAAjC,EAAoC;AAChCP,8BAAce,IAAd,CAAmBrB,oBAAoB,KAAKsB,MAAzB,EAAiC,EAAjC,EAAqCN,WAArC,EAAkDC,SAAlD,EAA6DC,SAA7D,EAAwEP,OAAxE,EAAiFQ,WAAjF,EAA8FC,SAA9F,CAAnB;AACA,qBAAKhB,WAAL,CAAiBmB,GAAjB;AACH;AACJ;;;2CAEkBjB,a,EAAekB,M,EAAQC,S,EAAW;AACjD,gBAAMC,kBAAkBD,UAAUZ,MAAlC;AACA,gBAAMc,kBAAkB7B,EAAEW,IAAF,CAAO,KAAKL,WAAZ,CAAxB;AACA,gBAAMwB,QAAQ,EAAd;AACA,gBAAMnB,OAAOX,EAAEW,IAAF,CAAOH,aAAP,CAAb;AACA,gBAAMU,cAAeP,IAAD,GAASA,KAAKQ,SAAL,GAAiB,CAA1B,GAA8B,CAAlD;AACA,gBAAMA,YAAYD,WAAlB;AACA,gBAAME,YAAY,KAAKW,cAAL,CAAoBvB,aAApB,EAAmCkB,OAAOM,EAA1C,CAAlB;AACA,gBAAMnB,UAAUO,SAAhB;AACA,gBAAMC,cAAeV,IAAD,GAASA,KAAKW,SAAL,GAAiB,CAA1B,GAA8B,CAAlD;AACA,gBAAMA,YAAYD,WAAlB;AACA,gBAAIO,kBAAkBC,eAAtB,EAAuC;AACnC,qBAAKvB,WAAL,CAAiBiB,IAAjB,CAAsBK,eAAtB;AACA,oBAAIK,cAAc/B,oBAAoB,KAAKgC,MAAzB,EAAiCJ,KAAjC,EAAwCZ,WAAxC,EAAqDC,SAArD,EAAgEC,SAAhE,EAA2EP,OAA3E,EAAoFQ,WAApF,EAAiGC,SAAjG,CAAlB;AACAd,8BAAce,IAAd,CAAmBU,WAAnB;AACH,aAJD,MAIO,IAAIL,kBAAkBC,eAAtB,EAAuC;AAC1C,uBAAO,KAAKvB,WAAL,CAAiBS,MAAjB,GAA0B,CAA1B,IAA+Ba,kBAAkB5B,EAAEW,IAAF,CAAO,KAAKL,WAAZ,CAAxD,EAAkF;AAC9E,yBAAKA,WAAL,CAAiBmB,GAAjB;AACA,wBAAIU,cAAcjC,oBAAoB,KAAKsB,MAAzB,EAAiCM,KAAjC,EAAwCZ,WAAxC,EAAqDC,SAArD,EAAgEC,SAAhE,EAA2EP,OAA3E,EAAoFQ,WAApF,EAAiGC,SAAjG,CAAlB;AACAd,kCAAce,IAAd,CAAmBY,WAAnB;AACH;AACJ;AACJ;;;AAED,4BAAc;AAAA;;AACV,YAAIC,IAAI,IAAR;AACAA,UAAEnB,MAAF,GAAW,EAAX,CAFU,CAEK;;AAEf,iBAASoB,aAAT,CAAuBC,IAAvB,EAA6BC,MAA7B,EAAqC/B,aAArC,EAAoDkB,MAApD,EAA4Dc,OAA5D,EAAqE;AACjE,gBAAIC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,kBAAkBC,IAAlB,CAAuBH,aAAvB,KAAyC,IAAjE;AACA,gBAAIzC,EAAEU,OAAF,CAAUF,aAAV,KAA4BmC,iBAAhC,EAAmD;AAC/C,oBAAIE,QAAQL,QAAQI,IAAR,CAAaH,aAAb,CAAZ;AACA,oBAAII,UAAU,IAAV,IAAkBA,MAAM9B,MAAN,IAAgB,CAAtC,EAAyC;AACrC,wBAAIY,YAAYkB,MAAM,CAAN,CAAhB;AACAT,sBAAEU,kBAAF,CAAqBtC,aAArB,EAAoCkB,MAApC,EAA4CC,SAA5C;AACA,2BAAOkB,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;AACD;AACA,YAAIE,uBAAuB/C,EAAEgD,YAAF,CAAeX,aAAf,EAA8B,iCAA9B,CAA3B;AACA,YAAIY,sBAAsBjD,EAAEgD,YAAF,CAAeX,aAAf,EAA8B,gCAA9B,CAA1B;AACA,YAAIa,uBAAuBlD,EAAEgD,YAAF,CAAeX,aAAf,EAA8B,kCAA9B,CAA3B;AACA,YAAIc,sBAAsBnD,EAAEgD,YAAF,CAAeX,aAAf,EAA8B,iCAA9B,CAA1B;AACA,YAAIe,qBAAqBpD,EAAEgD,YAAF,CAAeX,aAAf,EAA8B,gCAA9B,CAAzB;;AAEAD,UAAEiB,eAAF,GAAoBpD,YAAY;AAC5BqD,kBAAM,iBADsB;AAE5Bd,qBAASO;AAFmB,SAAZ,CAApB;AAIAX,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEiB,eAAhB;;AAEAjB,UAAEmB,cAAF,GAAmBtD,YAAY;AAC3BqD,kBAAM,gBADqB;AAE3Bd,qBAASS;AAFkB,SAAZ,CAAnB;AAIAb,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEmB,cAAhB;;AAEAnB,UAAEoB,eAAF,GAAoBvD,YAAY;AAC5BqD,kBAAM,iBADsB;AAE5Bd,qBAASU;AAFmB,SAAZ,CAApB;AAIAd,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEoB,eAAhB;;AAEApB,UAAEqB,cAAF,GAAmBxD,YAAY;AAC3BqD,kBAAM,gBADqB;AAE3Bd,qBAASW;AAFkB,SAAZ,CAAnB;AAIAf,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEqB,cAAhB;;AAEArB,UAAEsB,aAAF,GAAkBzD,YAAY;AAC1BqD,kBAAM,eADoB;AAE1Bd,qBAASY;AAFiB,SAAZ,CAAlB;AAIAhB,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEsB,aAAhB;;AAEA,YAAMC,wBAAwB,4BAA9B;;AAEA,iBAASC,mBAAT,CAA6BtB,IAA7B,EAAmCC,MAAnC,EAA2C/B,aAA3C,EAA0D;AACtD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,gBAAgBC,IAAhB,CAAqBH,aAArB,KAAuC,IAA/D;AACA,gBAAIzC,EAAEU,OAAF,CAAUF,aAAV,KAA4BmC,iBAAhC,EAAmD;AAC/C,oBAAIE,QAAQc,sBAAsBf,IAAtB,CAA2BH,aAA3B,CAAZ;AACA,oBAAGI,SAAS,IAAZ,EAAiB;AACfT,sBAAEyB,yBAAF,CAA4BrD,aAA5B;AACA,2BAAOqC,KAAP;AACD;AACJ;AACD,mBAAO,IAAP;AACH;AACDT,UAAE0B,cAAF,GAAmB7D,YAAY;AAC3BqD,kBAAM,gBADqB;AAE3Bd,qBAASoB,mBAFkB;AAG3BG,uBAAW;AAHgB,SAAZ,CAAnB;AAKA3B,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE0B,cAAhB;;AAEA1B,UAAE4B,KAAF,GAAU/D,YAAY;AAClBqD,kBAAM,OADY;AAElBd,qBAAS;AAFS,SAAZ,CAAV;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE4B,KAAhB;AACA5B,UAAE6B,aAAF,GAAkBhE,YAAY;AAC1BqD,kBAAM,eADoB;AAE1Bd,qBAAS;AAFiB,SAAZ,CAAlB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE6B,aAAhB;AACA7B,UAAE8B,oBAAF,GAAyBjE,YAAY;AACjCqD,kBAAM,sBAD2B;AAEjCd,qBAAS;AAFwB,SAAZ,CAAzB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE8B,oBAAhB;AACA9B,UAAE+B,aAAF,GAAkBlE,YAAY;AAC1BqD,kBAAM,eADoB;AAE1Bd,qBAAS;AAFiB,SAAZ,CAAlB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE+B,aAAhB;AACA/B,UAAEgC,WAAF,GAAgBnE,YAAY;AACxBqD,kBAAM,aADkB;AAExBd,qBAAS;AAFe,SAAZ,CAAhB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEgC,WAAhB;;AAEAhC,UAAEiC,YAAF,GAAiBpE,YAAY;AACzBqD,kBAAM,cADmB;AAEzBd,qBAAS,OAFgB;AAGzB8B,sBAAU;AAHe,SAAZ,CAAjB;AAKAlC,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEiC,YAAhB;;AAEA,iBAASE,aAAT,CAAuBjC,IAAvB,EAA6BC,MAA7B,EAAqC/B,aAArC,EAAoDkB,MAApD,EAA4Dc,OAA5D,EAAqE;AACjE,gBAAIC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,gBAAgBC,IAAhB,CAAqBH,aAArB,KAAuC,IAA/D;AACA,gBAAI9B,OAAOX,EAAEW,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAIgE,iBAAiB7D,QAAQR,aAAaQ,IAAb,EAAmByB,EAAEtB,SAArB,CAA7B;AACA,gBAAId,EAAEU,OAAF,CAAUF,aAAV,KAA4BgE,cAA5B,IAA8C7B,iBAAlD,EAAqE;AACjE,oBAAIE,QAAQL,QAAQI,IAAR,CAAaH,aAAb,CAAZ;AACA,oBAAII,UAAU,IAAd,EAAoB;AAChB,wBAAIlB,YAAYkB,MAAM,CAAN,CAAhB;AACAT,sBAAEU,kBAAF,CAAqBtC,aAArB,EAAoCkB,MAApC,EAA4CC,SAA5C;AACA,2BAAOkB,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;;AAED,YAAM4B,yBAAyB,uCAA/B;AACA,YAAIC,uBAAuB1E,EAAEgD,YAAF,CAAeuB,aAAf,EAA8BE,sBAA9B,CAA3B;;AAEArC,UAAEuC,eAAF,GAAoB1E,YAAY;AAC5BqD,kBAAM,iBADsB;AAE5Bd,qBAASkC;AAFmB,SAAZ,CAApB;AAIAtC,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEuC,eAAhB;AACA;AACA,YAAMC,2BAA2B,oCAAjC,CArIU,CAqI6D;AACvE,YAAIC,yBAAyB7E,EAAEgD,YAAF,CAAeuB,aAAf,EAA8BK,wBAA9B,CAA7B;;AAEAxC,UAAE0C,iBAAF,GAAsB7E,YAAY;AAC9BqD,kBAAM,mBADwB;AAE9Bd,qBAASqC;AAFqB,SAAZ,CAAtB;AAIAzC,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE0C,iBAAhB;;AAEA,YAAMC,gCAAgC,iCAAtC;;AAEA,iBAASC,2BAAT,CAAqC1C,IAArC,EAA2CC,MAA3C,EAAmD/B,aAAnD,EAAkE;AAC9D,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,kBAAkBC,IAAlB,CAAuBH,aAAvB,KAAyC,IAAjE;AACA,gBAAI9B,OAAOX,EAAEW,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAIgE,iBAAiB7D,QAAQR,aAAaQ,IAAb,EAAmByB,EAAEtB,SAArB,CAA7B;AACA,gBAAId,EAAEU,OAAF,CAAUF,aAAV,KAA4BgE,cAA5B,IAA8C7B,iBAAlD,EAAqE;AACjE,oBAAIE,QAAQkC,8BAA8BnC,IAA9B,CAAmCH,aAAnC,CAAZ;AACA,oBAAGI,KAAH,EAAS;AACPT,sBAAEyB,yBAAF,CAA4BrD,aAA5B;AACA,2BAAOqC,KAAP;AACD;AACJ;AACD,mBAAO,IAAP;AACH;;AAEDT,UAAE6C,sBAAF,GAA2BhF,YAAY;AACnCqD,kBAAM,wBAD6B;AAEnCd,qBAASwC;AAF0B,SAAZ,CAA3B;AAIA5C,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE6C,sBAAhB;;AAGA,YAAMC,mBAAmB,wCAAzB,CAtKU,CAsKyD;AACnE,iBAASC,cAAT,CAAwB7C,IAAxB,EAA8BC,MAA9B,EAAsC/B,aAAtC,EAAqD;AACjD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAI5B,OAAOX,EAAEW,IAAF,CAAOH,aAAP,CAAX;AACA;AACA,gBAAIG,QAAQR,aAAaQ,IAAb,EAAmByB,EAAEtB,SAArB,CAAZ,EACI,OAAO,IAAP;AACJ,gBAAI+B,QAAQqC,iBAAiBtC,IAAjB,CAAsBH,aAAtB,CAAZ;AACA,gBAAII,UAAU,IAAV,IAAkBA,MAAM,CAAN,EAAS9B,MAAT,GAAkB0B,cAAc1B,MAAtD,EAA8D;AAAE;AAC5DqB,kBAAEyB,yBAAF,CAA4BrD,aAA5B;AACA;AACA,uBAAOqC,KAAP;AACH;AACD,mBAAO,IAAP;AACH;AACDT,UAAEtB,SAAF,GAAcb,YAAY;AACtBqD,kBAAM,WADgB;AAEtBd,qBAAS2C;AAFa,SAAZ,CAAd;AAIA/C,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEtB,SAAhB;;AAEA;AACAsB,UAAEF,MAAF,GAAWjC,YAAY;AACnBqD,kBAAM,QADa;AAEnBd,qBAASzC,WAAWqF,KAAX,CAAiBC;AAFP,SAAZ,CAAX;AAIAjD,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEF,MAAhB;;AAEAE,UAAEZ,MAAF,GAAWvB,YAAY;AACnBqD,kBAAM,QADa;AAEnBd,qBAASzC,WAAWqF,KAAX,CAAiBC;AAFP,SAAZ,CAAX;AAIAjD,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEZ,MAAhB;;AAEAY,UAAEkD,mBAAF,GAAwBrF,YAAY;AAChCqD,kBAAM,qBAD0B;AAEhCd,qBAAS;AAFuB,SAAZ,CAAxB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEkD,mBAAhB;;AAEAlD,UAAEmD,kBAAF,GAAuBtF,YAAY;AAC/BqD,kBAAM,oBADyB;AAE/Bd,qBAAS;AAFsB,SAAZ,CAAvB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEmD,kBAAhB;;AAEAnD,UAAEoD,gBAAF,GAAqBvF,YAAY;AAC/BqD,kBAAM,kBADyB;AAE/Bd,qBAAS;AAFsB,SAAZ,CAArB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEoD,gBAAhB;;AAEApD,UAAEqD,kBAAF,GAAuBxF,YAAY;AAC/BqD,kBAAM,oBADyB;AAE/Bd,qBAAS;AAFsB,SAAZ,CAAvB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEqD,kBAAhB;;AAEArD,UAAEsD,iBAAF,GAAsBzF,YAAY;AAC9BqD,kBAAM,mBADwB;AAE9Bd,qBAAS;AAFqB,SAAZ,CAAtB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEsD,iBAAhB;;AAEAtD,UAAEuD,eAAF,GAAoB1F,YAAY;AAC9BqD,kBAAM,iBADwB;AAE9Bd,qBAAS;AAFqB,SAAZ,CAApB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEuD,eAAhB;;AAEA,YAAMC,iBAAiB,OAAvB;AACA,iBAASC,iBAAT,CAA2BvD,IAA3B,EAAiCC,MAAjC,EAAyC/B,aAAzC,EAAwD;AACpD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAI5B,OAAOX,EAAEW,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAIgE,iBAAiB7D,QAAQR,aAAaQ,IAAb,EAAmByB,EAAEtB,SAArB,CAA7B;;AAEA,gBAAI,CAACH,IAAD,IAAS6D,cAAb,EAA6B;AACzB,uBAAOoB,eAAehD,IAAf,CAAoBH,aAApB,CAAP;AACH;AACD,mBAAO,IAAP;AAEH;AACDL,UAAE0D,YAAF,GAAiB7F,YAAY;AACzBqD,kBAAM,cADmB;AAEzBd,qBAASqD;AAFgB,SAAZ,CAAjB;AAIAzD,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE0D,YAAhB;;AAEA;AACA,iBAASC,sBAAT,CAAgCzD,IAAhC,EAAsCC,MAAtC,EAA8C/B,aAA9C,EAA6DkB,MAA7D,EAAqEc,OAArE,EAA8EwD,SAA9E,EAAyF;AACrF,gBAAIvD,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAIM,QAAQL,QAAQI,IAAR,CAAaH,aAAb,CAAZ;AACA,gBAAII,SAAS,IAAb,EAAmB;AACfT,kBAAE7B,WAAF,CAAcgB,IAAd,CAAmByE,SAAnB;AACH;AACD,mBAAOnD,KAAP;AACH;;AAED,iBAASoD,oBAAT,CAA8B3D,IAA9B,EAAoCC,MAApC,EAA4C/B,aAA5C,EAA2DkB,MAA3D,EAAmEc,OAAnE,EAA4EwD,SAA5E,EAAuF;AACnF,gBAAIE,YAAYlG,EAAEW,IAAF,CAAOyB,EAAE7B,WAAT,CAAhB;AACA,gBAAI2F,aAAaF,SAAjB,EACI,OAAO,IAAP;AACJ;AACA,gBAAIG,UAAUzE,OAAO3B,WAAWqF,KAAX,CAAiBgB,OAAxB,CAAd;AACA,gBAAIC,cAAcrG,EAAEW,IAAF,CAAOwF,OAAP,CAAlB;AACA,gBAAIG,cAActG,EAAEW,IAAF,CAAOH,aAAP,CAAlB;AACA,gBAAI,CAAC8F,WAAD,IACCD,eAAetG,WAAWwG,YAAX,CAAwBF,WAAxB,IAAuCtG,WAAWwG,YAAX,CAAwBD,WAAxB,CAD3D,EACkG;AAC9F,uBAAO,IAAP;AACH;AACD,gBAAI7D,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAIM,QAAQL,QAAQI,IAAR,CAAaH,aAAb,CAAZ;;AAEA,gBAAII,SAAS,IAAb,EAAmB;AACfT,kBAAE7B,WAAF,CAAckB,GAAd;AACH;;AAED,mBAAOoB,KAAP;AACH;AACD,YAAI2D,yBAAyBxG,EAAEgD,YAAF,CAAe+C,sBAAf,EAAuC,aAAvC,EAAsD,cAAtD,CAA7B;AACA,YAAIU,uBAAuBzG,EAAEgD,YAAF,CAAeiD,oBAAf,EAAqC,8CAArC,EAAqF,cAArF,CAA3B;;AAEA,YAAIS,2BAA2B1G,EAAEgD,YAAF,CAAe+C,sBAAf,EAAuC,WAAvC,EAAoD,gBAApD,CAA/B;AACA,YAAIY,yBAAyB3G,EAAEgD,YAAF,CAAeiD,oBAAf,EAAqC,4CAArC,EAAmF,gBAAnF,CAA7B;;AAEA,YAAIW,2BAA2B5G,EAAEgD,YAAF,CAAe+C,sBAAf,EAAuC,WAAvC,EAAoD,gBAApD,CAA/B;AACA,YAAIc,yBAAyB7G,EAAEgD,YAAF,CAAeiD,oBAAf,EAAqC,4CAArC,EAAmF,gBAAnF,CAA7B;;AAEA,YAAIa,6BAA6B9G,EAAEgD,YAAF,CAAe+C,sBAAf,EAAuC,WAAvC,EAAoD,kBAApD,CAAjC;AACA,YAAIgB,2BAA2B/G,EAAEgD,YAAF,CAAeiD,oBAAf,EAAqC,4CAArC,EAAmF,kBAAnF,CAA/B;;AAGA7D,UAAE4E,iBAAF,GAAsB/G,YAAY;AAC9BqD,kBAAM,mBADwB;AAE9Bd,qBAASgE;AAFqB,SAAZ,CAAtB;AAIApE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE4E,iBAAhB;;AAEA5E,UAAE6E,eAAF,GAAoBhH,YAAY;AAC5BqD,kBAAM,iBADsB;AAE5Bd,qBAASiE;AAFmB,SAAZ,CAApB;AAIArE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE6E,eAAhB;;AAEA7E,UAAE8E,mBAAF,GAAwBjH,YAAY;AAChCqD,kBAAM,qBAD0B;AAEhCd,qBAASkE;AAFuB,SAAZ,CAAxB;AAIAtE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE8E,mBAAhB;;AAEA9E,UAAE+E,iBAAF,GAAsBlH,YAAY;AAC9BqD,kBAAM,mBADwB;AAE9Bd,qBAASmE;AAFqB,SAAZ,CAAtB;AAIAvE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE+E,iBAAhB;;AAEA/E,UAAEgF,mBAAF,GAAwBnH,YAAY;AAChCqD,kBAAM,qBAD0B;AAEhCd,qBAASoE;AAFuB,SAAZ,CAAxB;AAIAxE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEgF,mBAAhB;;AAEAhF,UAAEiF,iBAAF,GAAsBpH,YAAY;AAC9BqD,kBAAM,mBADwB;AAE9Bd,qBAASqE;AAFqB,SAAZ,CAAtB;AAIAzE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEiF,iBAAhB;;AAEAjF,UAAEkF,qBAAF,GAA0BrH,YAAY;AAClCqD,kBAAM,uBAD4B;AAElCd,qBAASsE;AAFyB,SAAZ,CAA1B;AAIA1E,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEkF,qBAAhB;;AAEAlF,UAAEmF,mBAAF,GAAwBtH,YAAY;AAChCqD,kBAAM,qBAD0B;AAEhCd,qBAASuE;AAFuB,SAAZ,CAAxB;AAIA3E,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEmF,mBAAhB;;AAEAnF,UAAEoF,OAAF,GAAYvH,YAAY;AACpBqD,kBAAM,SADc;AAEpBd,qBAAS,sBAFW;AAGpBiF,mBAAO1H,WAAWqF,KAAX,CAAiBgB;AAHJ,SAAZ,CAAZ;AAKAhE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEoF,OAAhB;;AAEApF,UAAEsF,IAAF,GAASzH,YAAY;AACjBqD,kBAAM,MADW;AAEjBd,qBAAS;AAFQ,SAAZ,CAAT;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEsF,IAAhB;;AAEAtF,UAAEuF,OAAF,GAAY1H,YAAY;AACpBqD,kBAAM,SADc;AAEpBd,qBAAS,gBAFW;AAGpBiF,mBAAO1H,WAAWqF,KAAX,CAAiBgB;AAHJ,SAAZ,CAAZ;AAKAhE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEuF,OAAhB;;AAEAvF,UAAEwF,MAAF,GAAW3H,YAAY;AACnBqD,kBAAM,QADa;AAEnBd,qBAAS,SAFU;AAGnBiF,mBAAO1H,WAAWqF,KAAX,CAAiBgB;AAHL,SAAZ,CAAX;AAKAhE,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEwF,MAAhB;;AAEA;AACAxF,UAAEyF,SAAF,GAAc5H,YAAY;AACtBqD,kBAAM,WADgB;AAEtBd,qBAAS;AAFa,SAAZ,CAAd;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAEyF,SAAhB;;AAEA;AACA;AACA;AACA;AACAzF,UAAE0F,iBAAF,GAAsB7H,YAAY;AAC9BqD,kBAAM,mBADwB;AAE9Bd,qBAAS;AAFqB,SAAZ,CAAtB;AAIAJ,UAAEnB,MAAF,CAASM,IAAT,CAAca,EAAE0F,iBAAhB;;AAEA,YAAIC,cAAc;AACdC,mBAAO;AACH,gCAAgB,CACZ5F,EAAEoF,OADU,EAEZpF,EAAEtB,SAFU;AAGZ;AACA;AACAsB,kBAAEZ,MALU,EAMZY,EAAEF,MANU,EAOZE,EAAE0B,cAPU,EAOM;AAClB1B,kBAAEiB,eARU,EASZjB,EAAEmB,cATU,EAUZnB,EAAEoB,eAVU,EAWZpB,EAAEqB,cAXU,EAYZrB,EAAEsB,aAZU,EAaZtB,EAAE0D,YAbU,EAcZ1D,EAAE6C,sBAdU,EAeZ7C,EAAEuC,eAfU,EAgBZvC,EAAE0C,iBAhBU;AAiBZ;AACA1C,kBAAE6E,eAlBU,EAkBO;AACnB7E,kBAAE+E,iBAnBU,EAmBS;AACrB/E,kBAAEiF,iBApBU,EAqBZjF,EAAEmF,mBArBU;AAsBZ;AACAnF,kBAAE4E,iBAvBU,EAuBS;AACrB5E,kBAAE8E,mBAxBU,EAwBW;AACvB9E,kBAAEgF,mBAzBU,EA0BZhF,EAAEkF,qBA1BU,EA2BZlF,EAAEsF,IA3BU,EA2BJ;AACRtF,kBAAEkD,mBA5BU,EA6BZlD,EAAEmD,kBA7BU,EA8BZnD,EAAEoD,gBA9BU,EA+BZpD,EAAEqD,kBA/BU,EAgCZrD,EAAEsD,iBAhCU,EAiCZtD,EAAEuD,eAjCU,EAkCZvD,EAAEuF,OAlCU,EAmCZvF,EAAEwF,MAnCU,EAoCZxF,EAAEyF,SApCU,EAqCZzF,EAAE0F,iBArCU,CADb;AAwCH,kCAAkB,CACd1F,EAAEoF,OADY,EAEdpF,EAAEiC,YAFY,EAGdjC,EAAE+B,aAHY,EAId/B,EAAEgC,WAJY,EAKdhC,EAAE8B,oBALY,EAMd9B,EAAE6B,aANY,EAOd7B,EAAE4B,KAPY,EAQd5B,EAAEuF,OARY,EASdvF,EAAEwF,MATY,EAUdxF,EAAEyF,SAVY,EAWdzF,EAAE0F,iBAXY;AAxCf,aADO;;AAwDdG,yBAAa;AAxDC,SAAlB;;AA2DA,aAAKC,MAAL,GAAc,IAAInI,WAAWqF,KAAf,CAAqB2C,WAArB,CAAd;AAEH;;;;uCACc9G,M,EAAQ;AACrB,gBAAIkH,MAAM,EAAV;AADqB;AAAA;AAAA;;AAAA;AAErB,qCAAkBlH,MAAlB,8HAA0B;AAAA,wBAAjBmH,KAAiB;;AACtBD,2BAAO/H,oBAAoBgI,KAApB,EAA2BC,SAA3B,GAAuC,GAAvC,GAA6CD,MAAMtG,KAAnD,GAA0D,IAAjE;AACH;AAJoB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAKrB,mBAAOqG,GAAP;AACD;;;iCACQ7F,I,EAAM;AACX,iBAAKgG,IAAL;;AAEA,gBAAIC,YAAY,KAAKL,MAAL,CAAYM,QAAZ,CAAqBlG,IAArB,CAAhB;;AAEA,iBAAKuB,yBAAL,CAA+B0E,UAAUtH,MAAzC;;AAEA,gBAAIsH,UAAUE,MAAV,CAAiB1H,MAAjB,GAA0B,CAA9B,EAAiC;AAC7B,sBAAM,IAAI2H,KAAJ,CAAU,sCAAV,CAAN;AACH;AACD,mBAAOH,SAAP;AACH;;;;;;AAGLI,OAAOC,OAAP,GAAiB;AACbvI,kBAAc,IAAIA,YAAJ;AADD,CAAjB","file":"ArgdownLexer.js","sourcesContent":["'use strict';\n\nimport * as chevrotain from 'chevrotain';\nimport * as _ from 'lodash';\n\nconst createToken = chevrotain.createToken;\nconst createTokenInstance = chevrotain.createTokenInstance;\nconst tokenMatcher = chevrotain.tokenMatcher;\nconst getTokenConstructor = chevrotain.getTokenConstructor;\n\nclass ArgdownLexer {\n    init() {\n        // State required for matching the indentations\n        this.indentStack = [0];\n        // State require for matching bold and italic ranges in the right order\n        this.rangesStack = [];\n    }\n    getCurrentLine(matchedTokens) {\n        const matchedTokensIsEmpty = _.isEmpty(matchedTokens);\n        if (matchedTokensIsEmpty)\n            return 0;\n\n        let last = _.last(matchedTokens);\n        let currentLine = (last)? last.endLine : 0;\n        if (last && chevrotain.tokenMatcher(last, this.Emptyline))\n            currentLine++;\n        return currentLine;\n    }\n\n    emitRemainingDedentTokens(matchedTokens) {\n        if (this.indentStack.length <= 1)\n            return;\n        const lastToken = _.last(matchedTokens.tokens);\n        const startOffset = (lastToken) ? lastToken.endOffset : 0;\n        const endOffset = startOffset;\n        const startLine = (lastToken) ? lastToken.endLine : 0;\n        const endLine = startLine;\n        const startColumn = (lastToken) ? lastToken.endColumn : 0;\n        const endColumn = startColumn;\n\n        //add remaining Dedents\n        while (this.indentStack.length > 1) {\n            matchedTokens.push(createTokenInstance(this.Dedent, \"\", startOffset, endOffset, startLine, endLine, startColumn, endColumn));\n            this.indentStack.pop();\n        }\n    }\n\n    emitIndentOrDedent(matchedTokens, groups, indentStr) {\n        const currIndentLevel = indentStr.length;\n        const lastIndentLevel = _.last(this.indentStack);\n        const image = \"\";\n        const last = _.last(matchedTokens);\n        const startOffset = (last) ? last.endOffset + 1 : 0;\n        const endOffset = startOffset;\n        const startLine = this.getCurrentLine(matchedTokens, groups.nl);\n        const endLine = startLine;\n        const startColumn = (last) ? last.endColumn + 1 : 0;\n        const endColumn = startColumn;\n        if (currIndentLevel > lastIndentLevel) {\n            this.indentStack.push(currIndentLevel);\n            let indentToken = createTokenInstance(this.Indent, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn);\n            matchedTokens.push(indentToken);\n        } else if (currIndentLevel < lastIndentLevel) {\n            while (this.indentStack.length > 1 && currIndentLevel < _.last(this.indentStack)) {\n                this.indentStack.pop();\n                let dedentToken = createTokenInstance(this.Dedent, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn);\n                matchedTokens.push(dedentToken);\n            }\n        }\n    }\n\n    constructor() {\n        let $ = this;\n        $.tokens = []; //token list for the parser\n\n        function matchRelation(text, offset, matchedTokens, groups, pattern) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^(?:\\n\\r|\\n|\\r)/.exec(remainingText) != null;\n            if (_.isEmpty(matchedTokens) || startsWithNewline) {\n                let match = pattern.exec(remainingText);\n                if (match !== null && match.length == 3) {\n                    let indentStr = match[1];\n                    $.emitIndentOrDedent(matchedTokens, groups, indentStr);\n                    return match;\n                }\n            }\n            return null;\n        }\n        //relations start at BOF or after a newline, optionally followed by indentation (spaces or tabs)\n        let matchIncomingSupport = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(\\+>)/);\n        let matchIncomingAttack = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(->)/);\n        let matchOutgoingSupport = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(<?\\+)/);\n        let matchOutgoingAttack = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(<?-)/);\n        let matchContradiction = _.partialRight(matchRelation, /^(?:\\n\\r|\\n|\\r)?([' '\\t]*)(><)/);\n\n        $.IncomingSupport = createToken({\n            name: \"IncomingSupport\",\n            pattern: matchIncomingSupport\n        });\n        $.tokens.push($.IncomingSupport);\n\n        $.IncomingAttack = createToken({\n            name: \"IncomingAttack\",\n            pattern: matchIncomingAttack\n        });\n        $.tokens.push($.IncomingAttack);\n\n        $.OutgoingSupport = createToken({\n            name: \"OutgoingSupport\",\n            pattern: matchOutgoingSupport\n        });\n        $.tokens.push($.OutgoingSupport);\n\n        $.OutgoingAttack = createToken({\n            name: \"OutgoingAttack\",\n            pattern: matchOutgoingAttack\n        });\n        $.tokens.push($.OutgoingAttack);\n\n        $.Contradiction = createToken({\n            name: \"Contradiction\",\n            pattern: matchContradiction\n        });\n        $.tokens.push($.Contradiction);\n\n        const inferenceStartPattern = /^[\\n\\r|\\n|\\r]?[' '\\t]*-{2}/;\n\n        function matchInferenceStart(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^[\\n\\r|\\n|\\r]/.exec(remainingText) != null;\n            if (_.isEmpty(matchedTokens) || startsWithNewline) {\n                let match = inferenceStartPattern.exec(remainingText);\n                if(match != null){\n                  $.emitRemainingDedentTokens(matchedTokens);\n                  return match;\n                }\n            }\n            return null;\n        }\n        $.InferenceStart = createToken({\n            name: \"InferenceStart\",\n            pattern: matchInferenceStart,\n            push_mode: \"inference_mode\"\n        });\n        $.tokens.push($.InferenceStart);\n\n        $.Colon = createToken({\n            name: \"Colon\",\n            pattern: /:/\n        });\n        $.tokens.push($.Colon);\n        $.ListDelimiter = createToken({\n            name: \"ListDelimiter\",\n            pattern: /,/\n        });\n        $.tokens.push($.ListDelimiter);\n        $.MetadataStatementEnd = createToken({\n            name: \"MetadataStatementEnd\",\n            pattern: /;/\n        });\n        $.tokens.push($.MetadataStatementEnd);\n        $.MetadataStart = createToken({\n            name: \"MetadataStart\",\n            pattern: /\\(/\n        });\n        $.tokens.push($.MetadataStart);\n        $.MetadataEnd = createToken({\n            name: \"MetadataEnd\",\n            pattern: /\\)/\n        });\n        $.tokens.push($.MetadataEnd);\n\n        $.InferenceEnd = createToken({\n            name: \"InferenceEnd\",\n            pattern: /-{2,}/,\n            pop_mode: true\n        });\n        $.tokens.push($.InferenceEnd);\n\n        function matchListItem(text, offset, matchedTokens, groups, pattern) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^[\\n\\r|\\n|\\r]/.exec(remainingText) != null;\n            let last = _.last(matchedTokens);\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) {\n                let match = pattern.exec(remainingText);\n                if (match !== null) {\n                    let indentStr = match[1];\n                    $.emitIndentOrDedent(matchedTokens, groups, indentStr);\n                    return match;\n                }\n            }\n            return null;\n        }\n\n        const orderedListItemPattern = /^(?:\\n\\r|\\n|\\r)?([' '\\t]+)\\d+\\.(?=\\s)/;\n        let matchOrderedListItem = _.partialRight(matchListItem, orderedListItemPattern);\n\n        $.OrderedListItem = createToken({\n            name: \"OrderedListItem\",\n            pattern: matchOrderedListItem\n        });\n        $.tokens.push($.OrderedListItem);\n        //whitespace + * + whitespace (to distinguish list items from bold and italic ranges)\n        const unorderedListItemPattern = /^(?:\\n\\r|\\n|\\r)?([' '\\t]+)\\*(?=\\s)/; //Newline +\n        let matchUnorderedListItem = _.partialRight(matchListItem, unorderedListItemPattern);\n\n        $.UnorderedListItem = createToken({\n            name: \"UnorderedListItem\",\n            pattern: matchUnorderedListItem\n        });\n        $.tokens.push($.UnorderedListItem);\n\n        const argumentStatementStartPattern = /^(?:\\n\\r|\\n|\\r)?[' '\\t]*\\(\\d+\\)/;\n\n        function matchArgumentStatementStart(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^(?:\\n\\r|\\n|\\r)/.exec(remainingText) != null;\n            let last = _.last(matchedTokens);\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) {\n                let match = argumentStatementStartPattern.exec(remainingText);\n                if(match){\n                  $.emitRemainingDedentTokens(matchedTokens);\n                  return match;\n                }\n            }\n            return null;\n        }\n\n        $.ArgumentStatementStart = createToken({\n            name: \"ArgumentStatementStart\",\n            pattern: matchArgumentStatementStart\n        });\n        $.tokens.push($.ArgumentStatementStart);\n\n\n        const emptylinePattern = /^((?:\\n\\r|\\n|\\r)[ \\t]*(?:\\n\\r|\\n|\\r)+)/; //two or more linebreaks\n        function matchEmptyline(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let last = _.last(matchedTokens);\n            //ignore Emptylines after first one (relevant for Emptylines after ignored comments)\n            if (last && tokenMatcher(last, $.Emptyline))\n                return null;\n            let match = emptylinePattern.exec(remainingText);\n            if (match !== null && match[0].length < remainingText.length) { //ignore trailing linebreaks\n                $.emitRemainingDedentTokens(matchedTokens);\n                //TODO: emitRemainingRanges (to be more resistant against unclosed bold and italic ranges)\n                return match;\n            }\n            return null;\n        }\n        $.Emptyline = createToken({\n            name: \"Emptyline\",\n            pattern: matchEmptyline\n        });\n        $.tokens.push($.Emptyline);\n\n        //Indent and Dedent are never matched with their own patterns, instead they get matched in the relations custom patterns\n        $.Indent = createToken({\n            name: \"Indent\",\n            pattern: chevrotain.Lexer.NA\n        });\n        $.tokens.push($.Indent);\n\n        $.Dedent = createToken({\n            name: \"Dedent\",\n            pattern: chevrotain.Lexer.NA\n        });\n        $.tokens.push($.Dedent);\n\n        $.StatementDefinition = createToken({\n            name: \"StatementDefinition\",\n            pattern: /\\[.+?\\]\\:/\n        });\n        $.tokens.push($.StatementDefinition);\n\n        $.StatementReference = createToken({\n            name: \"StatementReference\",\n            pattern: /\\[.+?\\]/\n        });\n        $.tokens.push($.StatementReference);\n\n        $.StatementMention = createToken({\n          name: \"StatementMention\",\n          pattern: /\\@\\[.+?\\][ \\t]?/\n        });\n        $.tokens.push($.StatementMention);\n\n        $.ArgumentDefinition = createToken({\n            name: \"ArgumentDefinition\",\n            pattern: /\\<.+?\\>\\:/\n        });\n        $.tokens.push($.ArgumentDefinition);\n\n        $.ArgumentReference = createToken({\n            name: \"ArgumentReference\",\n            pattern: /\\<.+?\\>/\n        });\n        $.tokens.push($.ArgumentReference);\n\n        $.ArgumentMention = createToken({\n          name: \"ArgumentMention\",\n          pattern: /\\@\\<.+?\\>[ \\t]?/\n        });\n        $.tokens.push($.ArgumentMention);\n\n        const headingPattern = /^(#+)/;\n        function matchHeadingStart(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let last = _.last(matchedTokens);\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n\n            if (!last || afterEmptyline) {\n                return headingPattern.exec(remainingText);\n            }\n            return null;\n\n        }\n        $.HeadingStart = createToken({\n            name: \"HeadingStart\",\n            pattern: matchHeadingStart\n        });\n        $.tokens.push($.HeadingStart);\n\n        //BOLD and ITALIC ranges\n        function matchBoldOrItalicStart(text, offset, matchedTokens, groups, pattern, rangeType) {\n            let remainingText = text.substr(offset);\n            let match = pattern.exec(remainingText);\n            if (match != null) {\n                $.rangesStack.push(rangeType);\n            }\n            return match;\n        }\n\n        function matchBoldOrItalicEnd(text, offset, matchedTokens, groups, pattern, rangeType) {\n            let lastRange = _.last($.rangesStack);\n            if (lastRange != rangeType)\n                return null;\n            //first check if the last match was skipped Whitespace\n            let skipped = groups[chevrotain.Lexer.SKIPPED];\n            let lastSkipped = _.last(skipped);\n            let lastMatched = _.last(matchedTokens);\n            if (!lastMatched ||\n                (lastSkipped && chevrotain.getEndOffset(lastSkipped) > chevrotain.getEndOffset(lastMatched))) {\n                return null;\n            }\n            let remainingText = text.substr(offset);\n            let match = pattern.exec(remainingText);\n\n            if (match != null) {\n                $.rangesStack.pop();\n            }\n\n            return match;\n        }\n        let matchAsteriskBoldStart = _.partialRight(matchBoldOrItalicStart, /^\\*\\*(?!\\s)/, \"AsteriskBold\");\n        let matchAsteriskBoldEnd = _.partialRight(matchBoldOrItalicEnd, /^\\*\\*(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"AsteriskBold\");\n\n        let matchUnderscoreBoldStart = _.partialRight(matchBoldOrItalicStart, /^__(?!\\s)/, \"UnderscoreBold\");\n        let matchUnderscoreBoldEnd = _.partialRight(matchBoldOrItalicEnd, /^__(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"UnderscoreBold\");\n\n        let matchAsteriskItalicStart = _.partialRight(matchBoldOrItalicStart, /^\\*(?!\\s)/, \"AsteriskItalic\");\n        let matchAsteriskItalicEnd = _.partialRight(matchBoldOrItalicEnd, /^\\*(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"AsteriskItalic\");\n\n        let matchUnderscoreItalicStart = _.partialRight(matchBoldOrItalicStart, /^\\_(?!\\s)/, \"UnderscoreItalic\");\n        let matchUnderscoreItalicEnd = _.partialRight(matchBoldOrItalicEnd, /^\\_(?:[ \\t]|(?=\\n|\\r|\\_|\\.|,|!|\\?|;|\\*|$))/, \"UnderscoreItalic\");\n\n\n        $.AsteriskBoldStart = createToken({\n            name: \"AsteriskBoldStart\",\n            pattern: matchAsteriskBoldStart\n        });\n        $.tokens.push($.AsteriskBoldStart);\n\n        $.AsteriskBoldEnd = createToken({\n            name: \"AsteriskBoldEnd\",\n            pattern: matchAsteriskBoldEnd\n        });\n        $.tokens.push($.AsteriskBoldEnd);\n\n        $.UnderscoreBoldStart = createToken({\n            name: \"UnderscoreBoldStart\",\n            pattern: matchUnderscoreBoldStart\n        });\n        $.tokens.push($.UnderscoreBoldStart);\n\n        $.UnderscoreBoldEnd = createToken({\n            name: \"UnderscoreBoldEnd\",\n            pattern: matchUnderscoreBoldEnd\n        });\n        $.tokens.push($.UnderscoreBoldEnd);\n\n        $.AsteriskItalicStart = createToken({\n            name: \"AsteriskItalicStart\",\n            pattern: matchAsteriskItalicStart\n        });\n        $.tokens.push($.AsteriskItalicStart);\n\n        $.AsteriskItalicEnd = createToken({\n            name: \"AsteriskItalicEnd\",\n            pattern: matchAsteriskItalicEnd\n        });\n        $.tokens.push($.AsteriskItalicEnd);\n\n        $.UnderscoreItalicStart = createToken({\n            name: \"UnderscoreItalicStart\",\n            pattern: matchUnderscoreItalicStart\n        });\n        $.tokens.push($.UnderscoreItalicStart);\n\n        $.UnderscoreItalicEnd = createToken({\n            name: \"UnderscoreItalicEnd\",\n            pattern: matchUnderscoreItalicEnd\n        });\n        $.tokens.push($.UnderscoreItalicEnd);\n\n        $.Comment = createToken({\n            name: \"Comment\",\n            pattern: /<!--(?:.|\\n|\\r)*?-->/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n        $.tokens.push($.Comment);\n\n        $.Link = createToken({\n            name: \"Link\",\n            pattern: /\\[[^\\]]+?\\]\\([^\\)]+?\\)[ \\t]?/\n        });\n        $.tokens.push($.Link);\n\n        $.Newline = createToken({\n            name: \"Newline\",\n            pattern: /(?:\\n\\r|\\n|\\r)/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n        $.tokens.push($.Newline);\n\n        $.Spaces = createToken({\n            name: \"Spaces\",\n            pattern: /( |\\t)+/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n        $.tokens.push($.Spaces);\n\n        //The rest of the text that is free of any Argdown syntax\n        $.Freestyle = createToken({\n            name: \"Freestyle\",\n            pattern: /[^\\@\\*\\_\\[\\]\\,\\:\\;\\<\\/\\>\\-\\n\\r\\(\\)]+/\n        });\n        $.tokens.push($.Freestyle);\n\n        //Freestyle text needs to be \"cut up\" by these control characters so that the other rules get a chance to succeed.\n        //Otherwise, every line would simply be lexed as a single Freestyle token.\n        //If these chars are not consumed by other rules, they are lexed as \"useless\" UnusedControlChars. The parser then has to combine Freestyle and UnusedControlChar tokens back together to get \"normal text\" token sequences.\n        //Note that some \"meaningful\" characters (like +) are not listed here, as they are only meaningful after a linebreak and freestyle text already gets \"cut up\" by each line break.\n        $.UnusedControlChar = createToken({\n            name: \"UnusedControlChar\",\n            pattern: /[\\@\\*\\_\\[\\]\\,\\:\\;\\<\\/\\>\\-\\(\\)][ \\t]?/\n        });\n        $.tokens.push($.UnusedControlChar);\n\n        let lexerConfig = {\n            modes: {\n                \"default_mode\": [\n                    $.Comment,\n                    $.Emptyline,\n                    // Relation tokens must appear before Spaces, otherwise all indentation will always be consumed as spaces.\n                    // Dedent must appear before Indent for handling zero spaces dedents.\n                    $.Dedent,\n                    $.Indent,\n                    $.InferenceStart, //needs to be lexed before OutgoingAttack (- vs --)\n                    $.IncomingSupport,\n                    $.IncomingAttack,\n                    $.OutgoingSupport,\n                    $.OutgoingAttack,\n                    $.Contradiction,\n                    $.HeadingStart,\n                    $.ArgumentStatementStart,\n                    $.OrderedListItem,\n                    $.UnorderedListItem,\n                    //The ends of Bold and italic ranges need to be lexed before the starts\n                    $.AsteriskBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (** vs *)\n                    $.UnderscoreBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (__ vs _)\n                    $.AsteriskItalicEnd,\n                    $.UnderscoreItalicEnd,\n                    //The starts of Bold and italic ranges need to be lexed after the ends\n                    $.AsteriskBoldStart, //BoldStart needs to be lexed before ItalicStart (** vs *)\n                    $.UnderscoreBoldStart, //BoldStart needs to be lexed before ItalicStart (__ vs _)\n                    $.AsteriskItalicStart,\n                    $.UnderscoreItalicStart,\n                    $.Link, //needs to be lexed before StatementReference\n                    $.StatementDefinition,\n                    $.StatementReference,\n                    $.StatementMention,\n                    $.ArgumentDefinition,\n                    $.ArgumentReference,\n                    $.ArgumentMention,\n                    $.Newline,\n                    $.Spaces,\n                    $.Freestyle,\n                    $.UnusedControlChar\n                ],\n                \"inference_mode\": [\n                    $.Comment,\n                    $.InferenceEnd,\n                    $.MetadataStart,\n                    $.MetadataEnd,\n                    $.MetadataStatementEnd,\n                    $.ListDelimiter,\n                    $.Colon,\n                    $.Newline,\n                    $.Spaces,\n                    $.Freestyle,\n                    $.UnusedControlChar\n                ]\n            },\n\n            defaultMode: \"default_mode\"\n        };\n\n        this._lexer = new chevrotain.Lexer(lexerConfig);\n\n    }\n    tokensToString(tokens) {\n      let str = \"\";\n      for (let token of tokens) {\n          str += getTokenConstructor(token).tokenName + \" \" + token.image +\"\\n\";\n      }\n      return str;\n    }\n    tokenize(text) {\n        this.init();\n\n        let lexResult = this._lexer.tokenize(text);\n\n        this.emitRemainingDedentTokens(lexResult.tokens);\n\n        if (lexResult.errors.length > 0) {\n            throw new Error(\"sad sad panda lexing errors detected\");\n        }\n        return lexResult;\n    }\n}\n\nmodule.exports = {\n    ArgdownLexer: new ArgdownLexer()\n};\n"]}