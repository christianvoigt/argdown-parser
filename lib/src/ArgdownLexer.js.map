{"version":3,"sources":["../../src/ArgdownLexer.js"],"names":["chevrotain","_","createToken","createTokenInstance","tokenMatcher","ArgdownLexer","indentStack","rangesStack","matchedTokens","matchedTokensIsEmpty","isEmpty","last","currentLine","endLine","Emptyline","length","lastToken","startOffset","endOffset","startLine","startColumn","endColumn","push","Dedent","pop","groups","indentStr","currIndentLevel","lastIndentLevel","image","getCurrentLine","nl","indentToken","Indent","dedentToken","$","tokens","matchRelation","text","offset","pattern","remainingText","substr","startsWithNewline","exec","afterEmptyline","match","emitIndentOrDedent","matchIncomingSupport","partialRight","matchIncomingAttack","matchOutgoingSupport","matchOutgoingAttack","matchContradiction","matchIncomingUndercut","matchOutgoingUndercut","IncomingSupport","name","line_breaks","label","IncomingAttack","OutgoingSupport","OutgoingAttack","Contradiction","IncomingUndercut","OutgoingUndercut","inferenceStartPattern","matchInferenceStart","emitRemainingDedentTokens","InferenceStart","push_mode","Colon","ListDelimiter","MetadataStatementEnd","MetadataStart","MetadataEnd","InferenceEnd","pop_mode","matchListItem","orderedListItemPattern","matchOrderedListItem","OrderedListItem","unorderedListItemPattern","matchUnorderedListItem","UnorderedListItem","emptylinePattern","matchEmptyline","Lexer","NA","StatementDefinition","StatementReference","StatementMention","statementNumberPattern","matchStatementNumber","StatementNumber","ArgumentDefinition","ArgumentReference","ArgumentMention","headingPattern","matchHeadingStart","HeadingStart","matchBoldOrItalicStart","rangeType","matchBoldOrItalicEnd","lastRange","skipped","SKIPPED","lastSkipped","lastMatched","getEndOffset","matchAsteriskBoldStart","matchAsteriskBoldEnd","matchUnderscoreBoldStart","matchUnderscoreBoldEnd","matchAsteriskItalicStart","matchAsteriskItalicEnd","matchUnderscoreItalicStart","matchUnderscoreItalicEnd","AsteriskBoldStart","AsteriskBoldEnd","UnderscoreBoldStart","UnderscoreBoldEnd","AsteriskItalicStart","AsteriskItalicEnd","UnderscoreItalicStart","UnderscoreItalicEnd","Comment","group","Link","Tag","Newline","Spaces","EscapedChar","Freestyle","UnusedControlChar","EOF","lexerConfig","modes","defaultMode","_lexer","str","token","tokenType","tokenName","init","lexResult","tokenize","errors","Error","module","exports"],"mappings":"AAAA;;;;AAEA;;IAAYA,U;;AACZ;;IAAYC,C;;;;;;AAEZ,IAAMC,cAAcF,WAAWE,WAA/B;AACA,IAAMC,sBAAsBH,WAAWG,mBAAvC;AACA,IAAMC,eAAeJ,WAAWI,YAAhC;;IAEMC,Y;;;+BACK;AACH;AACA,iBAAKC,WAAL,GAAmB,CAAC,CAAD,CAAnB;AACA;AACA,iBAAKC,WAAL,GAAmB,EAAnB;AACH;;;uCACcC,a,EAAe;AAC1B,gBAAMC,uBAAuBR,EAAES,OAAF,CAAUF,aAAV,CAA7B;AACA,gBAAIC,oBAAJ,EACI,OAAO,CAAP;;AAEJ,gBAAIE,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAII,cAAeD,IAAD,GAAQA,KAAKE,OAAb,GAAuB,CAAzC;AACA,gBAAIF,QAAQX,WAAWI,YAAX,CAAwBO,IAAxB,EAA8B,KAAKG,SAAnC,CAAZ,EACIF;AACJ,mBAAOA,WAAP;AACH;;;kDAEyBJ,a,EAAe;AACrC,gBAAI,KAAKF,WAAL,CAAiBS,MAAjB,IAA2B,CAA/B,EACI;AACJ,gBAAMC,YAAYf,EAAEU,IAAF,CAAOH,aAAP,CAAlB;AACA,gBAAMS,cAAeD,SAAD,GAAcA,UAAUE,SAAV,GAAsB,CAApC,GAAwC,CAA5D;AACA,gBAAMA,YAAYD,WAAlB;AACA,gBAAME,YAAaH,SAAD,GAAcA,UAAUH,OAAV,GAAoB,CAAlC,GAAsC,CAAxD,CANqC,CAMsB;AAC3D,gBAAMA,UAAUM,SAAhB;AACA,gBAAMC,cAAc,CAApB;AACA,gBAAMC,YAAYD,WAAlB;;AAEA;AACA,mBAAO,KAAKd,WAAL,CAAiBS,MAAjB,GAA0B,CAAjC,EAAoC;AAChCP,8BAAcc,IAAd,CAAmBnB,oBAAoB,KAAKoB,MAAzB,EAAiC,EAAjC,EAAqCN,WAArC,EAAkDC,SAAlD,EAA6DC,SAA7D,EAAwEN,OAAxE,EAAiFO,WAAjF,EAA8FC,SAA9F,CAAnB;AACA,qBAAKf,WAAL,CAAiBkB,GAAjB;AACH;AACJ;;;2CAEkBhB,a,EAAeiB,M,EAAQC,S,EAAW;AACjD,gBAAMC,kBAAkBD,UAAUX,MAAlC;AACA,gBAAMa,kBAAkB3B,EAAEU,IAAF,CAAO,KAAKL,WAAZ,CAAxB;AACA,gBAAMuB,QAAQ,EAAd;AACA,gBAAMlB,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAb;AACA,gBAAMS,cAAeN,IAAD,GAASA,KAAKO,SAAL,GAAiB,CAA1B,GAA8B,CAAlD,CALiD,CAKI;AACrD,gBAAMA,YAAYD,cAAcS,UAAUX,MAAxB,GAAiC,CAAnD;AACA,gBAAMI,YAAY,KAAKW,cAAL,CAAoBtB,aAApB,EAAmCiB,OAAOM,EAA1C,IAAgD,CAAlE,CAPiD,CAOmB;AACpE,gBAAMlB,UAAUM,SAAhB;AACA,gBAAMC,cAAc,CAApB;AACA,gBAAMC,YAAYD,cAAcM,UAAUX,MAAxB,GAAiC,CAAnD;AACA,gBAAIY,kBAAkBC,eAAtB,EAAuC;AACnC,qBAAKtB,WAAL,CAAiBgB,IAAjB,CAAsBK,eAAtB;AACA,oBAAIK,cAAc7B,oBAAoB,KAAK8B,MAAzB,EAAiCJ,KAAjC,EAAwCZ,WAAxC,EAAqDC,SAArD,EAAgEC,SAAhE,EAA2EN,OAA3E,EAAoFO,WAApF,EAAiGC,SAAjG,CAAlB;AACAb,8BAAcc,IAAd,CAAmBU,WAAnB;AACH,aAJD,MAIO,IAAIL,kBAAkBC,eAAtB,EAAuC;AAC1C,uBAAO,KAAKtB,WAAL,CAAiBS,MAAjB,GAA0B,CAA1B,IAA+BY,kBAAkB1B,EAAEU,IAAF,CAAO,KAAKL,WAAZ,CAAxD,EAAkF;AAC9E,yBAAKA,WAAL,CAAiBkB,GAAjB;AACA,wBAAIU,cAAc/B,oBAAoB,KAAKoB,MAAzB,EAAiCM,KAAjC,EAAwCZ,WAAxC,EAAqDC,SAArD,EAAgEC,SAAhE,EAA2EN,OAA3E,EAAoFO,WAApF,EAAiGC,SAAjG,CAAlB;AACAb,kCAAcc,IAAd,CAAmBY,WAAnB;AACH;AACJ;AACJ;;;AAED,4BAAc;AAAA;;AACV,YAAIC,IAAI,IAAR;AACAA,UAAEC,MAAF,GAAW,EAAX,CAFU,CAEK;;AAEf,iBAASC,aAAT,CAAuBC,IAAvB,EAA6BC,MAA7B,EAAqC/B,aAArC,EAAoDiB,MAApD,EAA4De,OAA5D,EAAqE;AACjE,gBAAIC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,kBAAkBC,IAAlB,CAAuBH,aAAvB,KAAyC,IAAjE;AACA,gBAAI9B,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAIqC,iBAAiBlC,QAAQP,aAAaO,IAAb,EAAmBwB,EAAErB,SAArB,CAA7B;;AAEA,gBAAIb,EAAES,OAAF,CAAUF,aAAV,KAA4BqC,cAA5B,IAA8CF,iBAAlD,EAAqE;AAAE;AACnE,oBAAIG,QAAQN,QAAQI,IAAR,CAAaH,aAAb,CAAZ;AACA,oBAAIK,UAAU,IAAV,IAAkBA,MAAM/B,MAAN,IAAgB,CAAtC,EAAyC;AACrC,wBAAIW,YAAYoB,MAAM,CAAN,CAAhB;AACAX,sBAAEY,kBAAF,CAAqBvC,aAArB,EAAoCiB,MAApC,EAA4CC,SAA5C;AACA,2BAAOoB,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;AACD;AACA,YAAIE,uBAAuB/C,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,iCAA9B,CAA3B;AACA,YAAIa,sBAAsBjD,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,gCAA9B,CAA1B;AACA,YAAIc,uBAAuBlD,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,kCAA9B,CAA3B;AACA,YAAIe,sBAAsBnD,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,iCAA9B,CAA1B;AACA,YAAIgB,qBAAqBpD,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,gCAA9B,CAAzB;AACA,YAAIiB,wBAAwBrD,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,gCAA9B,CAA5B;AACA,YAAIkB,wBAAwBtD,EAAEgD,YAAF,CAAeZ,aAAf,EAA8B,gCAA9B,CAA5B;;AAEAF,UAAEqB,eAAF,GAAoBtD,YAAY;AAC5BuD,kBAAM,iBADsB;AAE5BjB,qBAASQ,oBAFmB;AAG5BU,yBAAa,IAHe;AAI5BC,mBAAO;AAJqB,SAAZ,CAApB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEqB,eAAhB;;AAEArB,UAAEyB,cAAF,GAAmB1D,YAAY;AAC3BuD,kBAAM,gBADqB;AAE3BjB,qBAASU,mBAFkB;AAG3BQ,yBAAa,IAHc;AAI3BC,mBAAO;AAJoB,SAAZ,CAAnB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEyB,cAAhB;;AAEAzB,UAAE0B,eAAF,GAAoB3D,YAAY;AAC5BuD,kBAAM,iBADsB;AAE5BjB,qBAASW,oBAFmB;AAG5BO,yBAAa,IAHe;AAI5BC,mBAAO;AAJqB,SAAZ,CAApB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE0B,eAAhB;;AAEA1B,UAAE2B,cAAF,GAAmB5D,YAAY;AAC3BuD,kBAAM,gBADqB;AAE3BjB,qBAASY,mBAFkB;AAG3BM,yBAAa,IAHc;AAI3BC,mBAAO;AAJoB,SAAZ,CAAnB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE2B,cAAhB;;AAEA3B,UAAE4B,aAAF,GAAkB7D,YAAY;AAC1BuD,kBAAM,eADoB;AAE1BjB,qBAASa,kBAFiB;AAG1BK,yBAAa,IAHa;AAI1BC,mBAAO;AAJmB,SAAZ,CAAlB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE4B,aAAhB;AACA5B,UAAE6B,gBAAF,GAAqB9D,YAAY;AAC7BuD,kBAAM,kBADuB;AAE7BjB,qBAASc,qBAFoB;AAG7BI,yBAAa,IAHgB;AAI7BC,mBAAO;AAJsB,SAAZ,CAArB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE6B,gBAAhB;AACA7B,UAAE8B,gBAAF,GAAqB/D,YAAY;AAC7BuD,kBAAM,kBADuB;AAE7BjB,qBAASe,qBAFoB;AAG7BG,yBAAa,IAHgB;AAI7BC,mBAAO;AAJsB,SAAZ,CAArB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE8B,gBAAhB;;AAEA,YAAMC,wBAAwB,4BAA9B;;AAEA,iBAASC,mBAAT,CAA6B7B,IAA7B,EAAmCC,MAAnC,EAA2C/B,aAA3C,EAA0D;AACtD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,gBAAgBC,IAAhB,CAAqBH,aAArB,KAAuC,IAA/D;AACA,gBAAIxC,EAAES,OAAF,CAAUF,aAAV,KAA4BmC,iBAAhC,EAAmD;AAC/C,oBAAIG,QAAQoB,sBAAsBtB,IAAtB,CAA2BH,aAA3B,CAAZ;AACA,oBAAGK,SAAS,IAAZ,EAAiB;AACfX,sBAAEiC,yBAAF,CAA4B5D,aAA5B;AACA,2BAAOsC,KAAP;AACD;AACJ;AACD,mBAAO,IAAP;AACH;AACDX,UAAEkC,cAAF,GAAmBnE,YAAY;AAC3BuD,kBAAM,gBADqB;AAE3BjB,qBAAS2B,mBAFkB;AAG3BG,uBAAW,gBAHgB;AAI3BZ,yBAAa,IAJc;AAK3BC,mBAAO;AALoB,SAAZ,CAAnB;AAOAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEkC,cAAhB;;AAEAlC,UAAEoC,KAAF,GAAUrE,YAAY;AAClBuD,kBAAM,OADY;AAElBjB,qBAAS,GAFS;AAGlBmB,mBAAO;AAHW,SAAZ,CAAV;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEoC,KAAhB;AACApC,UAAEqC,aAAF,GAAkBtE,YAAY;AAC1BuD,kBAAM,eADoB;AAE1BjB,qBAAS,GAFiB;AAG1BmB,mBAAO;AAHmB,SAAZ,CAAlB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEqC,aAAhB;AACArC,UAAEsC,oBAAF,GAAyBvE,YAAY;AACjCuD,kBAAM,sBAD2B;AAEjCjB,qBAAS,GAFwB;AAGjCmB,mBAAO;AAH0B,SAAZ,CAAzB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEsC,oBAAhB;AACAtC,UAAEuC,aAAF,GAAkBxE,YAAY;AAC1BuD,kBAAM,eADoB;AAE1BjB,qBAAS,IAFiB;AAG1BmB,mBAAO;AAHmB,SAAZ,CAAlB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEuC,aAAhB;AACAvC,UAAEwC,WAAF,GAAgBzE,YAAY;AACxBuD,kBAAM,aADkB;AAExBjB,qBAAS,IAFe;AAGxBmB,mBAAO;AAHiB,SAAZ,CAAhB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEwC,WAAhB;;AAEAxC,UAAEyC,YAAF,GAAiB1E,YAAY;AACzBuD,kBAAM,cADmB;AAEzBjB,qBAAS,OAFgB;AAGzBqC,sBAAU,IAHe;AAIzBlB,mBAAO;AAJkB,SAAZ,CAAjB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEyC,YAAhB;;AAEA,iBAASE,aAAT,CAAuBxC,IAAvB,EAA6BC,MAA7B,EAAqC/B,aAArC,EAAoDiB,MAApD,EAA4De,OAA5D,EAAqE;AACjE,gBAAIC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAII,oBAAoB,gBAAgBC,IAAhB,CAAqBH,aAArB,KAAuC,IAA/D;AACA,gBAAI9B,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAIqC,iBAAiBlC,QAAQP,aAAaO,IAAb,EAAmBwB,EAAErB,SAArB,CAA7B;AACA,gBAAIb,EAAES,OAAF,CAAUF,aAAV,KAA4BqC,cAA5B,IAA8CF,iBAAlD,EAAqE;AACjE,oBAAIG,QAAQN,QAAQI,IAAR,CAAaH,aAAb,CAAZ;AACA,oBAAIK,UAAU,IAAd,EAAoB;AAChB,wBAAIpB,YAAYoB,MAAM,CAAN,CAAhB;AACAX,sBAAEY,kBAAF,CAAqBvC,aAArB,EAAoCiB,MAApC,EAA4CC,SAA5C;AACA,2BAAOoB,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;;AAED,YAAMiC,yBAAyB,uCAA/B;AACA,YAAIC,uBAAuB/E,EAAEgD,YAAF,CAAe6B,aAAf,EAA8BC,sBAA9B,CAA3B;;AAEA5C,UAAE8C,eAAF,GAAoB/E,YAAY;AAC5BuD,kBAAM,iBADsB;AAE5BjB,qBAASwC,oBAFmB;AAG5BtB,yBAAa,IAHe;AAI5BC,mBAAO;AAJqB,SAAZ,CAApB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE8C,eAAhB;AACA;AACA,YAAMC,2BAA2B,oCAAjC,CA5KU,CA4K6D;AACvE,YAAIC,yBAAyBlF,EAAEgD,YAAF,CAAe6B,aAAf,EAA8BI,wBAA9B,CAA7B;;AAEA/C,UAAEiD,iBAAF,GAAsBlF,YAAY;AAC9BuD,kBAAM,mBADwB;AAE9BjB,qBAAS2C,sBAFqB;AAG9BzB,yBAAa,IAHiB;AAI9BC,mBAAO;AAJuB,SAAZ,CAAtB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEiD,iBAAhB;;AAEA;AACA;AACA,YAAMC,mBAAmB,kCAAzB,CAzLU,CAyLmD;AAC7D,iBAASC,cAAT,CAAwBhD,IAAxB,EAA8BC,MAA9B,EAAsC/B,aAAtC,EAAqD;AACjD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAI5B,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAX;AACA;AACA,gBAAIG,QAAQP,aAAaO,IAAb,EAAmBwB,EAAErB,SAArB,CAAZ,EACI,OAAO,IAAP;AACJ,gBAAIgC,QAAQuC,iBAAiBzC,IAAjB,CAAsBH,aAAtB,CAAZ;AACA,gBAAIK,UAAU,IAAV,IAAkBA,MAAM,CAAN,EAAS/B,MAAT,GAAkB0B,cAAc1B,MAAtD,EAA8D;AAAE;AAC5DoB,kBAAEiC,yBAAF,CAA4B5D,aAA5B;AACA;AACA,uBAAOsC,KAAP;AACH;AACD,mBAAO,IAAP;AACH;AACDX,UAAErB,SAAF,GAAcZ,YAAY;AACtBuD,kBAAM,WADgB;AAEtBjB,qBAAS8C,cAFa;AAGtB5B,yBAAa,IAHS;AAItBC,mBAAO;AAJe,SAAZ,CAAd;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAErB,SAAhB;;AAEA;AACAqB,UAAEF,MAAF,GAAW/B,YAAY;AACnBuD,kBAAM,QADa;AAEnBjB,qBAASxC,WAAWuF,KAAX,CAAiBC;AAFP,SAAZ,CAAX;AAIArD,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEF,MAAhB;;AAEAE,UAAEZ,MAAF,GAAWrB,YAAY;AACnBuD,kBAAM,QADa;AAEnBjB,qBAASxC,WAAWuF,KAAX,CAAiBC;AAFP,SAAZ,CAAX;AAIArD,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEZ,MAAhB;;AAEAY,UAAEsD,mBAAF,GAAwBvF,YAAY;AAChCuD,kBAAM,qBAD0B;AAEhCjB,qBAAS,WAFuB;AAGhCmB,mBAAO;AAHyB,SAAZ,CAAxB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEsD,mBAAhB;;AAEA;AACA;AACA;AACA;AACA;;AAEAtD,UAAEuD,kBAAF,GAAuBxF,YAAY;AAC/BuD,kBAAM,oBADyB;AAE/BjB,qBAAS,SAFsB;AAG/BmB,mBAAO;AAHwB,SAAZ,CAAvB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEuD,kBAAhB;;AAEA;AACA;AACA;AACA;AACA;;;AAGAvD,UAAEwD,gBAAF,GAAqBzF,YAAY;AAC/BuD,kBAAM,kBADyB;AAE/BjB,qBAAS,iBAFsB;AAG/BmB,mBAAO;AAHwB,SAAZ,CAArB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEwD,gBAAhB;;AAEA;AACA;AACA;AACA;AACA;;AAEA,YAAMC,yBAAyB,iCAA/B;AACA,iBAASC,oBAAT,CAA8BvD,IAA9B,EAAoCC,MAApC,EAA4C/B,aAA5C,EAA2D;AACvD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAI5B,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAImC,oBAAoB,kBAAkBC,IAAlB,CAAuBH,aAAvB,KAAyC,IAAjE;AACA,gBAAII,iBAAiBlC,QAAQP,aAAaO,IAAb,EAAmBwB,EAAErB,SAArB,CAA7B;;AAEA;AACA,gBAAIb,EAAES,OAAF,CAAUF,aAAV,KAA4BqC,cAA5B,IAA8CF,iBAAlD,EAAqE;AACjE,oBAAIG,QAAQ8C,uBAAuBhD,IAAvB,CAA4BH,aAA5B,CAAZ;AACA,oBAAIK,UAAU,IAAd,EAAoB;AAChBX,sBAAEiC,yBAAF,CAA4B5D,aAA5B;AACA,2BAAOsC,KAAP;AACH;AACJ;AACD,mBAAO,IAAP;AACH;AACDX,UAAE2D,eAAF,GAAoB5F,YAAY;AAC5BuD,kBAAM,iBADsB;AAE5BjB,qBAASqD,oBAFmB;AAG5BnC,yBAAa,IAHe;AAI5BC,mBAAO;AAJqB,SAAZ,CAApB;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE2D,eAAhB;;AAEA3D,UAAE4D,kBAAF,GAAuB7F,YAAY;AAC/BuD,kBAAM,oBADyB;AAE/BjB,qBAAS,WAFsB;AAG/BmB,mBAAO;AAHwB,SAAZ,CAAvB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE4D,kBAAhB;;AAEA5D,UAAE6D,iBAAF,GAAsB9F,YAAY;AAC9BuD,kBAAM,mBADwB;AAE9BjB,qBAAS,SAFqB;AAG9BmB,mBAAO;AAHuB,SAAZ,CAAtB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE6D,iBAAhB;;AAEA7D,UAAE8D,eAAF,GAAoB/F,YAAY;AAC9BuD,kBAAM,iBADwB;AAE9BjB,qBAAS,iBAFqB;AAG9BmB,mBAAO;AAHuB,SAAZ,CAApB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE8D,eAAhB;;AAEA,YAAMC,iBAAiB,YAAvB;AACA,iBAASC,iBAAT,CAA2B7D,IAA3B,EAAiCC,MAAjC,EAAyC/B,aAAzC,EAAwD;AACpD,gBAAIiC,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAI5B,OAAOV,EAAEU,IAAF,CAAOH,aAAP,CAAX;AACA,gBAAIqC,iBAAiBlC,QAAQP,aAAaO,IAAb,EAAmBwB,EAAErB,SAArB,CAA7B;;AAEA,gBAAI,CAACH,IAAD,IAASkC,cAAb,EAA6B;AACzB,uBAAOqD,eAAetD,IAAf,CAAoBH,aAApB,CAAP;AACH;AACD,mBAAO,IAAP;AAEH;AACDN,UAAEiE,YAAF,GAAiBlG,YAAY;AACzBuD,kBAAM,cADmB;AAEzBjB,qBAAS2D,iBAFgB;AAGzBxC,mBAAO;AAHkB,SAAZ,CAAjB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEiE,YAAhB;;AAEA;AACA,iBAASC,sBAAT,CAAgC/D,IAAhC,EAAsCC,MAAtC,EAA8C/B,aAA9C,EAA6DiB,MAA7D,EAAqEe,OAArE,EAA8E8D,SAA9E,EAAyF;AACrF,gBAAI7D,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAIO,QAAQN,QAAQI,IAAR,CAAaH,aAAb,CAAZ;AACA,gBAAIK,SAAS,IAAb,EAAmB;AACfX,kBAAE5B,WAAF,CAAce,IAAd,CAAmBgF,SAAnB;AACH;AACD,mBAAOxD,KAAP;AACH;;AAED,iBAASyD,oBAAT,CAA8BjE,IAA9B,EAAoCC,MAApC,EAA4C/B,aAA5C,EAA2DiB,MAA3D,EAAmEe,OAAnE,EAA4E8D,SAA5E,EAAuF;AACnF,gBAAIE,YAAYvG,EAAEU,IAAF,CAAOwB,EAAE5B,WAAT,CAAhB;AACA,gBAAIiG,aAAaF,SAAjB,EACI,OAAO,IAAP;AACJ;AACA,gBAAIG,UAAUhF,OAAOzB,WAAWuF,KAAX,CAAiBmB,OAAxB,CAAd;AACA,gBAAIC,cAAc1G,EAAEU,IAAF,CAAO8F,OAAP,CAAlB;AACA,gBAAIG,cAAc3G,EAAEU,IAAF,CAAOH,aAAP,CAAlB;AACA,gBAAI,CAACoG,WAAD,IACCD,eAAe3G,WAAW6G,YAAX,CAAwBF,WAAxB,IAAuC3G,WAAW6G,YAAX,CAAwBD,WAAxB,CAD3D,EACkG;AAC9F,uBAAO,IAAP;AACH;AACD,gBAAInE,gBAAgBH,KAAKI,MAAL,CAAYH,MAAZ,CAApB;AACA,gBAAIO,QAAQN,QAAQI,IAAR,CAAaH,aAAb,CAAZ;;AAEA,gBAAIK,SAAS,IAAb,EAAmB;AACfX,kBAAE5B,WAAF,CAAciB,GAAd;AACH;;AAED,mBAAOsB,KAAP;AACH;AACD,YAAIgE,yBAAyB7G,EAAEgD,YAAF,CAAeoD,sBAAf,EAAuC,aAAvC,EAAsD,cAAtD,CAA7B;AACA,YAAIU,uBAAuB9G,EAAEgD,YAAF,CAAesD,oBAAf,EAAqC,wDAArC,EAA+F,cAA/F,CAA3B;;AAEA,YAAIS,2BAA2B/G,EAAEgD,YAAF,CAAeoD,sBAAf,EAAuC,WAAvC,EAAoD,gBAApD,CAA/B;AACA,YAAIY,yBAAyBhH,EAAEgD,YAAF,CAAesD,oBAAf,EAAqC,sDAArC,EAA6F,gBAA7F,CAA7B;;AAEA,YAAIW,2BAA2BjH,EAAEgD,YAAF,CAAeoD,sBAAf,EAAuC,WAAvC,EAAoD,gBAApD,CAA/B;AACA,YAAIc,yBAAyBlH,EAAEgD,YAAF,CAAesD,oBAAf,EAAqC,sDAArC,EAA6F,gBAA7F,CAA7B;;AAEA,YAAIa,6BAA6BnH,EAAEgD,YAAF,CAAeoD,sBAAf,EAAuC,WAAvC,EAAoD,kBAApD,CAAjC;AACA,YAAIgB,2BAA2BpH,EAAEgD,YAAF,CAAesD,oBAAf,EAAqC,sDAArC,EAA6F,kBAA7F,CAA/B;;AAGApE,UAAEmF,iBAAF,GAAsBpH,YAAY;AAC9BuD,kBAAM,mBADwB;AAE9BjB,qBAASsE,sBAFqB;AAG9BnD,mBAAO;AAHuB,SAAZ,CAAtB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEmF,iBAAhB;;AAEAnF,UAAEoF,eAAF,GAAoBrH,YAAY;AAC5BuD,kBAAM,iBADsB;AAE5BjB,qBAASuE,oBAFmB;AAG5BpD,mBAAO;AAHqB,SAAZ,CAApB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEoF,eAAhB;;AAEApF,UAAEqF,mBAAF,GAAwBtH,YAAY;AAChCuD,kBAAM,qBAD0B;AAEhCjB,qBAASwE,wBAFuB;AAGhCrD,mBAAO;AAHyB,SAAZ,CAAxB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEqF,mBAAhB;;AAEArF,UAAEsF,iBAAF,GAAsBvH,YAAY;AAC9BuD,kBAAM,mBADwB;AAE9BjB,qBAASyE,sBAFqB;AAG9BtD,mBAAO;AAHuB,SAAZ,CAAtB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEsF,iBAAhB;;AAEAtF,UAAEuF,mBAAF,GAAwBxH,YAAY;AAChCuD,kBAAM,qBAD0B;AAEhCjB,qBAAS0E,wBAFuB;AAGhCvD,mBAAO;AAHyB,SAAZ,CAAxB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEuF,mBAAhB;;AAEAvF,UAAEwF,iBAAF,GAAsBzH,YAAY;AAC9BuD,kBAAM,mBADwB;AAE9BjB,qBAAS2E,sBAFqB;AAG9BxD,mBAAO;AAHuB,SAAZ,CAAtB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEwF,iBAAhB;;AAEAxF,UAAEyF,qBAAF,GAA0B1H,YAAY;AAClCuD,kBAAM,uBAD4B;AAElCjB,qBAAS4E,0BAFyB;AAGlCzD,mBAAO;AAH2B,SAAZ,CAA1B;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEyF,qBAAhB;;AAEAzF,UAAE0F,mBAAF,GAAwB3H,YAAY;AAChCuD,kBAAM,qBAD0B;AAEhCjB,qBAAS6E,wBAFuB;AAGhC1D,mBAAO;AAHyB,SAAZ,CAAxB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE0F,mBAAhB;;AAEA1F,UAAE2F,OAAF,GAAY5H,YAAY;AACpBuD,kBAAM,SADc;AAEpBjB,qBAAS,8EAFW;AAGpBuF,mBAAO/H,WAAWuF,KAAX,CAAiBmB,OAHJ;AAIpBhD,yBAAa,IAJO;AAKpBC,mBAAO;AALa,SAAZ,CAAZ;AAOAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE2F,OAAhB;;AAEA3F,UAAE6F,IAAF,GAAS9H,YAAY;AACjBuD,kBAAM,MADW;AAEjBjB,qBAAS,8BAFQ;AAGjBmB,mBAAO;AAHU,SAAZ,CAAT;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE6F,IAAhB;;AAEA7F,UAAE8F,GAAF,GAAQ/H,YAAY;AAClBuD,kBAAM,KADY;AAElBjB,qBAAS,4EAFS;AAGlBmB,mBAAO;AAHW,SAAZ,CAAR;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE8F,GAAhB;;AAEA9F,UAAE+F,OAAF,GAAYhI,YAAY;AACpBuD,kBAAM,SADc;AAEpBjB,qBAAS,gBAFW;AAGpBuF,mBAAO/H,WAAWuF,KAAX,CAAiBmB,OAHJ;AAIpBhD,yBAAa,IAJO;AAKpBC,mBAAO;AALa,SAAZ,CAAZ;AAOAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAE+F,OAAhB;;AAEA/F,UAAEgG,MAAF,GAAWjI,YAAY;AACnBuD,kBAAM,QADa;AAEnBjB,qBAAS,SAFU;AAGnBuF,mBAAO/H,WAAWuF,KAAX,CAAiBmB;AAHL,SAAZ,CAAX;AAKAvE,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEgG,MAAhB;;AAEAhG,UAAEiG,WAAF,GAAgBlI,YAAY;AACxBuD,kBAAM,aADkB;AAExBjB,qBAAS,WAFe;AAGxBmB,mBAAO;AAHiB,SAAZ,CAAhB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEiG,WAAhB;;AAEA;AACAjG,UAAEkG,SAAF,GAAcnI,YAAY;AACtBuD,kBAAM,WADgB;AAEtBjB,qBAAS,0CAFa;AAGtBkB,yBAAa,IAHS;AAItBC,mBAAO;AAJe,SAAZ,CAAd;AAMAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEkG,SAAhB;;AAEA;AACA;AACA;AACA;AACAlG,UAAEmG,iBAAF,GAAsBpI,YAAY;AAC9BuD,kBAAM,mBADwB;AAE9BjB,qBAAS,wCAFqB;AAG9BmB,mBAAO;AAHuB,SAAZ,CAAtB;AAKAxB,UAAEC,MAAF,CAASd,IAAT,CAAca,EAAEmG,iBAAhB;;AAEAnG,UAAEoG,GAAF,GAAQvI,WAAWuI,GAAnB;;AAEA,YAAIC,cAAc;AACdC,mBAAO;AACH,gCAAgB,CACZtG,EAAE2F,OADU,EAEZ3F,EAAEiG,WAFU,EAEG;AACfjG,kBAAErB,SAHU;AAIZ;AACA;AACAqB,kBAAEZ,MANU,EAOZY,EAAEF,MAPU,EAQZE,EAAEkC,cARU,EAQM;AAClBlC,kBAAEqB,eATU,EAUZrB,EAAEyB,cAVU,EAWZzB,EAAE0B,eAXU,EAYZ1B,EAAE2B,cAZU,EAaZ3B,EAAE4B,aAbU,EAcZ5B,EAAE6B,gBAdU,EAeZ7B,EAAE8B,gBAfU,EAgBZ9B,EAAEiE,YAhBU;AAiBZ;AACAjE,kBAAE2D,eAlBU,EAmBZ3D,EAAE8C,eAnBU,EAoBZ9C,EAAEiD,iBApBU;AAqBZ;AACAjD,kBAAEoF,eAtBU,EAsBO;AACnBpF,kBAAEsF,iBAvBU,EAuBS;AACrBtF,kBAAEwF,iBAxBU,EAyBZxF,EAAE0F,mBAzBU;AA0BZ;AACA1F,kBAAEmF,iBA3BU,EA2BS;AACrBnF,kBAAEqF,mBA5BU,EA4BW;AACvBrF,kBAAEuF,mBA7BU,EA8BZvF,EAAEyF,qBA9BU,EA+BZzF,EAAE6F,IA/BU,EA+BJ;AACR7F,kBAAE8F,GAhCU;AAiCZ;AACA;AACA;AACA9F,kBAAEsD,mBApCU,EAqCZtD,EAAEuD,kBArCU,EAsCZvD,EAAEwD,gBAtCU,EAuCZxD,EAAE4D,kBAvCU,EAwCZ5D,EAAE6D,iBAxCU,EAyCZ7D,EAAE8D,eAzCU,EA0CZ9D,EAAE+F,OA1CU,EA2CZ/F,EAAEgG,MA3CU,EA4CZhG,EAAEkG,SA5CU,EA6CZlG,EAAEmG,iBA7CU,CADb;AAgDH,kCAAkB,CACdnG,EAAE2F,OADY,EAEd3F,EAAEyC,YAFY,EAGdzC,EAAEuC,aAHY,EAIdvC,EAAEwC,WAJY,EAKdxC,EAAEsC,oBALY,EAMdtC,EAAEqC,aANY,EAOdrC,EAAEoC,KAPY,EAQdpC,EAAE+F,OARY,EASd/F,EAAEgG,MATY,EAUdhG,EAAEkG,SAVY,EAWdlG,EAAEmG,iBAXY;AAhDf,aADO;;AAgEdI,yBAAa;AAhEC,SAAlB;;AAmEA,aAAKC,MAAL,GAAc,IAAI3I,WAAWuF,KAAf,CAAqBiD,WAArB,CAAd;AAEH;;;;uCACcpG,M,EAAQ;AACrB,gBAAIwG,MAAM,EAAV;AADqB;AAAA;AAAA;;AAAA;AAErB,qCAAkBxG,MAAlB,8HAA0B;AAAA,wBAAjByG,KAAiB;;AACtBD,2BAAOC,MAAMC,SAAN,CAAgBC,SAAhB,GAA4B,GAA5B,GAAkCF,MAAMhH,KAAxC,GAA+C,IAAtD;AACH;AAJoB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAKrB,mBAAO+G,GAAP;AACD;;;+CACsBxG,M,EAAO;AAC1B,gBAAIwG,MAAM,EAAV;AAD0B;AAAA;AAAA;;AAAA;AAE1B,sCAAkBxG,MAAlB,mIAA0B;AAAA,wBAAjByG,KAAiB;;AACtBD,2BAAOC,MAAMC,SAAN,CAAgBC,SAAhB,GAA4B,GAA5B,GAAkCF,MAAMhH,KAAxC,GAAgD,IAAvD;AACA+G,2BAAO,kBAAkBC,MAAM5H,WAAxB,GAAsC,cAAtC,GAAuD4H,MAAM3H,SAA7D,GAAwE,cAAxE,GAAuF2H,MAAM1H,SAA7F,GAAuG,YAAvG,GAAoH0H,MAAMhI,OAA1H,GAAkI,gBAAlI,GAAmJgI,MAAMzH,WAAzJ,GAAqK,cAArK,GAAoLyH,MAAMxH,SAA1L,GAAoM,MAA3M;AACH;AALyB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAM1B,mBAAOuH,GAAP;AACH;;;iCACQtG,I,EAAM;AACX,iBAAK0G,IAAL;;AAEA,gBAAIC,YAAY,KAAKN,MAAL,CAAYO,QAAZ,CAAqB5G,IAArB,CAAhB;AACA,gBAAI2G,UAAUE,MAAV,CAAiBpI,MAAjB,GAA0B,CAA9B,EAAiC;AAC7B,sBAAM,IAAIqI,KAAJ,CAAU,sCAAV,CAAN;AACH;;AAED;AACA,gBAAGhJ,aAAaH,EAAEU,IAAF,CAAOsI,UAAU7G,MAAjB,CAAb,EAAuC,KAAKtB,SAA5C,CAAH,EAA0D;AACxDmI,0BAAU7G,MAAV,CAAiBZ,GAAjB;AACD;;AAED,iBAAK4C,yBAAL,CAA+B6E,UAAU7G,MAAzC;;AAEA,mBAAO6G,SAAP;AACH;;;;;;AAGLI,OAAOC,OAAP,GAAiB;AACbjJ,kBAAc,IAAIA,YAAJ;AADD,CAAjB","file":"ArgdownLexer.js","sourcesContent":["'use strict';\n\nimport * as chevrotain from 'chevrotain';\nimport * as _ from 'lodash';\n\nconst createToken = chevrotain.createToken;\nconst createTokenInstance = chevrotain.createTokenInstance;\nconst tokenMatcher = chevrotain.tokenMatcher;\n\nclass ArgdownLexer {\n    init() {\n        // State required for matching the indentations\n        this.indentStack = [0];\n        // State require for matching bold and italic ranges in the right order\n        this.rangesStack = [];\n    }\n    getCurrentLine(matchedTokens) {\n        const matchedTokensIsEmpty = _.isEmpty(matchedTokens);\n        if (matchedTokensIsEmpty)\n            return 1;\n\n        let last = _.last(matchedTokens);\n        let currentLine = (last)? last.endLine : 1;\n        if (last && chevrotain.tokenMatcher(last, this.Emptyline))\n            currentLine++;\n        return currentLine;\n    }\n\n    emitRemainingDedentTokens(matchedTokens) {\n        if (this.indentStack.length <= 1)\n            return;\n        const lastToken = _.last(matchedTokens);\n        const startOffset = (lastToken) ? lastToken.endOffset + 1 : 0; \n        const endOffset = startOffset;\n        const startLine = (lastToken) ? lastToken.endLine + 1 : 1; //+1 because of linebreak before indentation\n        const endLine = startLine;\n        const startColumn = 1;\n        const endColumn = startColumn;\n\n        //add remaining Dedents\n        while (this.indentStack.length > 1) {\n            matchedTokens.push(createTokenInstance(this.Dedent, \"\", startOffset, endOffset, startLine, endLine, startColumn, endColumn));\n            this.indentStack.pop();\n        }\n    }\n\n    emitIndentOrDedent(matchedTokens, groups, indentStr) {\n        const currIndentLevel = indentStr.length;\n        const lastIndentLevel = _.last(this.indentStack);\n        const image = \"\";\n        const last = _.last(matchedTokens);\n        const startOffset = (last) ? last.endOffset + 2 : 0; //+2 because of linebreak before indentation\n        const endOffset = startOffset + indentStr.length - 1;\n        const startLine = this.getCurrentLine(matchedTokens, groups.nl) + 1;//relation includes linebreak\n        const endLine = startLine; \n        const startColumn = 1;\n        const endColumn = startColumn + indentStr.length - 1;\n        if (currIndentLevel > lastIndentLevel) {\n            this.indentStack.push(currIndentLevel);\n            let indentToken = createTokenInstance(this.Indent, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn);\n            matchedTokens.push(indentToken);\n        } else if (currIndentLevel < lastIndentLevel) {\n            while (this.indentStack.length > 1 && currIndentLevel < _.last(this.indentStack)) {\n                this.indentStack.pop();\n                let dedentToken = createTokenInstance(this.Dedent, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn);\n                matchedTokens.push(dedentToken);\n            }\n        }\n    }\n\n    constructor() {\n        let $ = this;\n        $.tokens = []; //token list for the parser\n\n        function matchRelation(text, offset, matchedTokens, groups, pattern) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^(?:\\r\\n|\\n|\\r)/.exec(remainingText) != null;\n            let last = _.last(matchedTokens);\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n            \n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) { //relations after Emptyline are illegal, but we need the token for error reporting\n                let match = pattern.exec(remainingText);\n                if (match !== null && match.length == 3) {\n                    let indentStr = match[1];\n                    $.emitIndentOrDedent(matchedTokens, groups, indentStr);\n                    return match;\n                }\n            }\n            return null;\n        }\n        //relations start at BOF or after a newline, optionally followed by indentation (spaces or tabs)\n        let matchIncomingSupport = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(\\+>)/);\n        let matchIncomingAttack = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(->)/);\n        let matchOutgoingSupport = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(<?\\+)/);\n        let matchOutgoingAttack = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(<?-)/);\n        let matchContradiction = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(><)/);\n        let matchIncomingUndercut = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(_>)/);\n        let matchOutgoingUndercut = _.partialRight(matchRelation, /^(?:\\r\\n|\\n|\\r)?([' '\\t]*)(<_)/);\n\n        $.IncomingSupport = createToken({\n            name: \"IncomingSupport\",\n            pattern: matchIncomingSupport,\n            line_breaks: true,\n            label: \"+> (Incoming Support)\"\n        });\n        $.tokens.push($.IncomingSupport);\n\n        $.IncomingAttack = createToken({\n            name: \"IncomingAttack\",\n            pattern: matchIncomingAttack,\n            line_breaks: true,\n            label: \"-> (Incoming Attack)\"\n        });\n        $.tokens.push($.IncomingAttack);\n\n        $.OutgoingSupport = createToken({\n            name: \"OutgoingSupport\",\n            pattern: matchOutgoingSupport,\n            line_breaks: true,\n            label: \"<+ (Outgoing Support)\"\n        });\n        $.tokens.push($.OutgoingSupport);\n\n        $.OutgoingAttack = createToken({\n            name: \"OutgoingAttack\",\n            pattern: matchOutgoingAttack,\n            line_breaks: true,\n            label: \"<- (Outgoing Attack)\"\n        });\n        $.tokens.push($.OutgoingAttack);\n\n        $.Contradiction = createToken({\n            name: \"Contradiction\",\n            pattern: matchContradiction,\n            line_breaks: true,\n            label: \">< (Contradiction)\"\n        });\n        $.tokens.push($.Contradiction);\n        $.IncomingUndercut = createToken({\n            name: \"IncomingUndercut\",\n            pattern: matchIncomingUndercut,\n            line_breaks: true,\n            label: \"_> (Incoming Undercut)\"\n        });\n        $.tokens.push($.IncomingUndercut);\n        $.OutgoingUndercut = createToken({\n            name: \"OutgoingUndercut\",\n            pattern: matchOutgoingUndercut,\n            line_breaks: true,\n            label: \"<_ (Outgoing Undercut)\"\n        });\n        $.tokens.push($.OutgoingUndercut);\n\n        const inferenceStartPattern = /^[\\r\\n|\\n|\\r]?[' '\\t]*-{2}/;\n\n        function matchInferenceStart(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^[\\r\\n|\\n|\\r]/.exec(remainingText) != null;\n            if (_.isEmpty(matchedTokens) || startsWithNewline) {\n                let match = inferenceStartPattern.exec(remainingText);\n                if(match != null){\n                  $.emitRemainingDedentTokens(matchedTokens);\n                  return match;\n                }\n            }\n            return null;\n        }\n        $.InferenceStart = createToken({\n            name: \"InferenceStart\",\n            pattern: matchInferenceStart,\n            push_mode: \"inference_mode\",\n            line_breaks: true,\n            label: \"-- (Inference Start)\"            \n        });\n        $.tokens.push($.InferenceStart);\n\n        $.Colon = createToken({\n            name: \"Colon\",\n            pattern: /:/,\n            label: \":\"\n        });\n        $.tokens.push($.Colon);\n        $.ListDelimiter = createToken({\n            name: \"ListDelimiter\",\n            pattern: /,/,\n            label: \",\"\n        });\n        $.tokens.push($.ListDelimiter);\n        $.MetadataStatementEnd = createToken({\n            name: \"MetadataStatementEnd\",\n            pattern: /;/,\n            label: \";\"\n        });\n        $.tokens.push($.MetadataStatementEnd);\n        $.MetadataStart = createToken({\n            name: \"MetadataStart\",\n            pattern: /\\(/,\n            label: \"(\"\n        });\n        $.tokens.push($.MetadataStart);\n        $.MetadataEnd = createToken({\n            name: \"MetadataEnd\",\n            pattern: /\\)/,\n            label: \")\"\n        });\n        $.tokens.push($.MetadataEnd);\n\n        $.InferenceEnd = createToken({\n            name: \"InferenceEnd\",\n            pattern: /-{2,}/,\n            pop_mode: true,\n            label: \"-- (Inference End)\"\n        });\n        $.tokens.push($.InferenceEnd);\n\n        function matchListItem(text, offset, matchedTokens, groups, pattern) {\n            let remainingText = text.substr(offset);\n            let startsWithNewline = /^[\\r\\n|\\n|\\r]/.exec(remainingText) != null;\n            let last = _.last(matchedTokens);\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) {\n                let match = pattern.exec(remainingText);\n                if (match !== null) {\n                    let indentStr = match[1];\n                    $.emitIndentOrDedent(matchedTokens, groups, indentStr);\n                    return match;\n                }\n            }\n            return null;\n        }\n\n        const orderedListItemPattern = /^(?:\\r\\n|\\n|\\r)?([' '\\t]+)\\d+\\.(?=\\s)/;\n        let matchOrderedListItem = _.partialRight(matchListItem, orderedListItemPattern);\n\n        $.OrderedListItem = createToken({\n            name: \"OrderedListItem\",\n            pattern: matchOrderedListItem,\n            line_breaks: true,\n            label: \"{Indentation}{number}. (Ordered List Item)\"         \n        });\n        $.tokens.push($.OrderedListItem);\n        //whitespace + * + whitespace (to distinguish list items from bold and italic ranges)\n        const unorderedListItemPattern = /^(?:\\r\\n|\\n|\\r)?([' '\\t]+)\\*(?=\\s)/; //Newline +\n        let matchUnorderedListItem = _.partialRight(matchListItem, unorderedListItemPattern);\n\n        $.UnorderedListItem = createToken({\n            name: \"UnorderedListItem\",\n            pattern: matchUnorderedListItem,\n            line_breaks: true,\n            label: \"{Indentation}* (Unordered List Item)\"           \n        });\n        $.tokens.push($.UnorderedListItem);\n\n        //This does not work with \\r\\n|\\n||r as a simple CRLF linebreak will be interpreted as an Emptyline\n        //Instead we drop the last alternative (\\r?\\n would work as well)\n        const emptylinePattern = /^((?:\\r\\n|\\n)[ \\t]*(?:\\r\\n|\\n)+)/; //two or more linebreaks\n        function matchEmptyline(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let last = _.last(matchedTokens);\n            //ignore Emptylines after first one (relevant for Emptylines after ignored comments)\n            if (last && tokenMatcher(last, $.Emptyline))\n                return null;\n            let match = emptylinePattern.exec(remainingText);\n            if (match !== null && match[0].length < remainingText.length) { //ignore trailing linebreaks\n                $.emitRemainingDedentTokens(matchedTokens);\n                //TODO: emitRemainingRanges (to be more resistant against unclosed bold and italic ranges)\n                return match;\n            }\n            return null;\n        }\n        $.Emptyline = createToken({\n            name: \"Emptyline\",\n            pattern: matchEmptyline,\n            line_breaks: true,\n            label: \"{linebreak}{linebreak} (Empty Line)\"       \n        });\n        $.tokens.push($.Emptyline);\n\n        //Indent and Dedent are never matched with their own patterns, instead they get matched in the relations custom patterns\n        $.Indent = createToken({\n            name: \"Indent\",\n            pattern: chevrotain.Lexer.NA\n        });\n        $.tokens.push($.Indent);\n\n        $.Dedent = createToken({\n            name: \"Dedent\",\n            pattern: chevrotain.Lexer.NA\n        });\n        $.tokens.push($.Dedent);\n\n        $.StatementDefinition = createToken({\n            name: \"StatementDefinition\",\n            pattern: /\\[.+?\\]\\:/,\n            label: \"[Statement Title]: (Statement Definition)\"\n        });\n        $.tokens.push($.StatementDefinition);\n\n        // $.StatementDefinitionByNumber = createToken({\n        //     name: \"StatementDefinitionByNumber\",\n        //     pattern: /\\<(.+?)\\>\\((\\d+)\\)\\:/\n        // });\n        // $.tokens.push($.StatementDefinitionByNumber);\n\n        $.StatementReference = createToken({\n            name: \"StatementReference\",\n            pattern: /\\[.+?\\]/,\n            label: \"[Statement Title] (Statement Reference)\"\n        });\n        $.tokens.push($.StatementReference);\n\n        // $.StatementReferenceByNumber = createToken({\n        //     name: \"StatementReferenceByNumber\",\n        //     pattern: /\\<(.+?)\\>\\(\\d+\\)/\n        // });\n        // $.tokens.push($.StatementReferenceByNumber);\n\n\n        $.StatementMention = createToken({\n          name: \"StatementMention\",\n          pattern: /\\@\\[.+?\\][ \\t]?/,\n          label: \"@[Statement Title] (Statement Mention)\"\n        });\n        $.tokens.push($.StatementMention);\n\n        // $.StatementMentionByNumber = createToken({\n        //     name: \"StatementMentionByNumber\",\n        //     pattern: /\\@\\<(.+?)\\>\\(\\d+\\)/\n        // });\n        // $.tokens.push($.StatementMentionByNumber);\n\n        const statementNumberPattern = /^(?:\\r\\n|\\n|\\r)?[' '\\t]*\\(\\d+\\)/;\n        function matchStatementNumber(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            var last = _.last(matchedTokens);\n            let startsWithNewline = /^(?:\\r\\n|\\n|\\r)/.exec(remainingText) != null;\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n\n            //Statement in argument reconstruction:\n            if (_.isEmpty(matchedTokens) || afterEmptyline || startsWithNewline) {\n                let match = statementNumberPattern.exec(remainingText);\n                if (match !== null) {\n                    $.emitRemainingDedentTokens(matchedTokens);\n                    return match;\n                }\n            }\n            return null;\n        }\n        $.StatementNumber = createToken({\n            name: \"StatementNumber\",\n            pattern: matchStatementNumber,\n            line_breaks: true,\n            label: \"(Number) (Statement Number)\"   \n        });\n        $.tokens.push($.StatementNumber);\n\n        $.ArgumentDefinition = createToken({\n            name: \"ArgumentDefinition\",\n            pattern: /\\<.+?\\>\\:/,\n            label: \"<Argument Title>: (Argument Definition)\"\n        });\n        $.tokens.push($.ArgumentDefinition);\n\n        $.ArgumentReference = createToken({\n            name: \"ArgumentReference\",\n            pattern: /\\<.+?\\>/,\n            label: \"<Argument Title> (Argument Reference)\"\n        });\n        $.tokens.push($.ArgumentReference);\n\n        $.ArgumentMention = createToken({\n          name: \"ArgumentMention\",\n          pattern: /\\@\\<.+?\\>[ \\t]?/,\n          label: \"@<Argument Title> (Argument Mention)\"\n        });\n        $.tokens.push($.ArgumentMention);\n\n        const headingPattern = /^(#+)(?: )/;\n        function matchHeadingStart(text, offset, matchedTokens) {\n            let remainingText = text.substr(offset);\n            let last = _.last(matchedTokens);\n            let afterEmptyline = last && tokenMatcher(last, $.Emptyline);\n\n            if (!last || afterEmptyline) {\n                return headingPattern.exec(remainingText);\n            }\n            return null;\n\n        }\n        $.HeadingStart = createToken({\n            name: \"HeadingStart\",\n            pattern: matchHeadingStart,\n            label: \"# (Heading Start)\"\n        });\n        $.tokens.push($.HeadingStart);\n\n        //BOLD and ITALIC ranges\n        function matchBoldOrItalicStart(text, offset, matchedTokens, groups, pattern, rangeType) {\n            let remainingText = text.substr(offset);\n            let match = pattern.exec(remainingText);\n            if (match != null) {\n                $.rangesStack.push(rangeType);\n            }\n            return match;\n        }\n\n        function matchBoldOrItalicEnd(text, offset, matchedTokens, groups, pattern, rangeType) {\n            let lastRange = _.last($.rangesStack);\n            if (lastRange != rangeType)\n                return null;\n            //first check if the last match was skipped Whitespace\n            let skipped = groups[chevrotain.Lexer.SKIPPED];\n            let lastSkipped = _.last(skipped);\n            let lastMatched = _.last(matchedTokens);\n            if (!lastMatched ||\n                (lastSkipped && chevrotain.getEndOffset(lastSkipped) > chevrotain.getEndOffset(lastMatched))) {\n                return null;\n            }\n            let remainingText = text.substr(offset);\n            let match = pattern.exec(remainingText);\n\n            if (match != null) {\n                $.rangesStack.pop();\n            }\n\n            return match;\n        }\n        let matchAsteriskBoldStart = _.partialRight(matchBoldOrItalicStart, /^\\*\\*(?!\\s)/, \"AsteriskBold\");\n        let matchAsteriskBoldEnd = _.partialRight(matchBoldOrItalicEnd, /^\\*\\*(?:[ \\t]|(?=\\n|\\r|\\)|\\}|\\_|\\.|,|!|\\?|;|:|-|\\*|$))/, \"AsteriskBold\");\n\n        let matchUnderscoreBoldStart = _.partialRight(matchBoldOrItalicStart, /^__(?!\\s)/, \"UnderscoreBold\");\n        let matchUnderscoreBoldEnd = _.partialRight(matchBoldOrItalicEnd, /^__(?:[ \\t]|(?=\\n|\\r|\\)|\\}|\\_|\\.|,|!|\\?|;|:|-|\\*|$))/, \"UnderscoreBold\");\n\n        let matchAsteriskItalicStart = _.partialRight(matchBoldOrItalicStart, /^\\*(?!\\s)/, \"AsteriskItalic\");\n        let matchAsteriskItalicEnd = _.partialRight(matchBoldOrItalicEnd, /^\\*(?:[ \\t]|(?=\\n|\\r|\\)|\\}|\\_|\\.|,|!|\\?|;|:|-|\\*|$))/, \"AsteriskItalic\");\n\n        let matchUnderscoreItalicStart = _.partialRight(matchBoldOrItalicStart, /^\\_(?!\\s)/, \"UnderscoreItalic\");\n        let matchUnderscoreItalicEnd = _.partialRight(matchBoldOrItalicEnd, /^\\_(?:[ \\t]|(?=\\n|\\r|\\)|\\}|\\_|\\.|,|!|\\?|;|:|-|\\*|$))/, \"UnderscoreItalic\");\n\n\n        $.AsteriskBoldStart = createToken({\n            name: \"AsteriskBoldStart\",\n            pattern: matchAsteriskBoldStart,\n            label: \"** (Bold Start)\"\n        });\n        $.tokens.push($.AsteriskBoldStart);\n\n        $.AsteriskBoldEnd = createToken({\n            name: \"AsteriskBoldEnd\",\n            pattern: matchAsteriskBoldEnd,\n            label: \"** (Bold End)\"\n        });\n        $.tokens.push($.AsteriskBoldEnd);\n\n        $.UnderscoreBoldStart = createToken({\n            name: \"UnderscoreBoldStart\",\n            pattern: matchUnderscoreBoldStart,\n            label: \"__ (Bold Start)\"\n        });\n        $.tokens.push($.UnderscoreBoldStart);\n\n        $.UnderscoreBoldEnd = createToken({\n            name: \"UnderscoreBoldEnd\",\n            pattern: matchUnderscoreBoldEnd,\n            label: \"__ (Bold End)\"\n        });\n        $.tokens.push($.UnderscoreBoldEnd);\n\n        $.AsteriskItalicStart = createToken({\n            name: \"AsteriskItalicStart\",\n            pattern: matchAsteriskItalicStart,\n            label: \"* (Italic Start)\"\n        });\n        $.tokens.push($.AsteriskItalicStart);\n\n        $.AsteriskItalicEnd = createToken({\n            name: \"AsteriskItalicEnd\",\n            pattern: matchAsteriskItalicEnd,\n            label: \"* (Italic End)\"\n        });\n        $.tokens.push($.AsteriskItalicEnd);\n\n        $.UnderscoreItalicStart = createToken({\n            name: \"UnderscoreItalicStart\",\n            pattern: matchUnderscoreItalicStart,\n            label: \"_ (Italic Start)\"\n        });\n        $.tokens.push($.UnderscoreItalicStart);\n\n        $.UnderscoreItalicEnd = createToken({\n            name: \"UnderscoreItalicEnd\",\n            pattern: matchUnderscoreItalicEnd,\n            label: \"_ (Italic End)\"\n        });\n        $.tokens.push($.UnderscoreItalicEnd);\n\n        $.Comment = createToken({\n            name: \"Comment\",\n            pattern: /(?:<!--(?:.|\\n|\\r)*?-->)|(?:\\/\\*(?:.|\\n|\\r)*?\\*\\/)|(?:\\/\\/.*?(?=\\r\\n|\\n|\\r))/,\n            group: chevrotain.Lexer.SKIPPED,\n            line_breaks: true,\n            label: \"// or /**/ or <!-- --> (Comment)\"        \n        });\n        $.tokens.push($.Comment);\n\n        $.Link = createToken({\n            name: \"Link\",\n            pattern: /\\[[^\\]]+?\\]\\([^\\)]+?\\)[ \\t]?/,\n            label: \"[Title](Url) (Link)\"\n        });\n        $.tokens.push($.Link);\n        \n        $.Tag = createToken({\n          name: \"Tag\",\n          pattern: /#(?:\\([^\\)]+\\)|[a-zA-z0-9-\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]+)[ \\t]?/,\n          label: \"#tag-text or #(tag text) (Tag)\"\n        });\n        $.tokens.push($.Tag);\n\n        $.Newline = createToken({\n            name: \"Newline\",\n            pattern: /(?:\\r\\n|\\n|\\r)/,\n            group: chevrotain.Lexer.SKIPPED,\n            line_breaks: true,\n            label: \"{linebreak} (New Line)\"\n        });\n        $.tokens.push($.Newline);\n\n        $.Spaces = createToken({\n            name: \"Spaces\",\n            pattern: /( |\\t)+/,\n            group: chevrotain.Lexer.SKIPPED\n        });\n        $.tokens.push($.Spaces);\n\n        $.EscapedChar = createToken({\n            name: \"EscapedChar\",\n            pattern: /\\\\.(?: )*/,\n            label: \"\\\\{character} (Escaped Character)\"\n        });\n        $.tokens.push($.EscapedChar);\n\n        //The rest of the text that is free of any Argdown syntax\n        $.Freestyle = createToken({\n            name: \"Freestyle\",\n            pattern: /[^\\\\\\@\\#\\*\\_\\[\\]\\,\\:\\;\\<\\/\\>\\-\\r\\n\\(\\)]+/,\n            line_breaks: true,\n            label: \"Text Content\"\n        });\n        $.tokens.push($.Freestyle);\n\n        //Freestyle text needs to be \"cut up\" by these control characters so that the other rules get a chance to succeed.\n        //Otherwise, every line would simply be lexed as a single Freestyle token.\n        //If these chars are not consumed by other rules, they are lexed as \"useless\" UnusedControlChars. The parser then has to combine Freestyle and UnusedControlChar tokens back together to get \"normal text\" token sequences.\n        //Note that some \"meaningful\" characters (like +) are not listed here, as they are only meaningful after a linebreak and freestyle text already gets \"cut up\" by each line break.\n        $.UnusedControlChar = createToken({\n            name: \"UnusedControlChar\",\n            pattern: /[\\@\\#\\*\\_\\[\\]\\,\\:\\;\\<\\/\\>\\-\\(\\)][ \\t]?/,\n            label: \"Text Content (Special Characters)\"\n        });\n        $.tokens.push($.UnusedControlChar);\n\n        $.EOF = chevrotain.EOF;\n\n        let lexerConfig = {\n            modes: {\n                \"default_mode\": [\n                    $.Comment,\n                    $.EscapedChar, //must come first after $.Comment\n                    $.Emptyline,\n                    // Relation tokens must appear before Spaces, otherwise all indentation will always be consumed as spaces.\n                    // Dedent must appear before Indent for handling zero spaces dedents.\n                    $.Dedent,\n                    $.Indent,\n                    $.InferenceStart, //needs to be lexed before OutgoingAttack (- vs --)\n                    $.IncomingSupport,\n                    $.IncomingAttack,\n                    $.OutgoingSupport,\n                    $.OutgoingAttack,\n                    $.Contradiction,\n                    $.IncomingUndercut,\n                    $.OutgoingUndercut,\n                    $.HeadingStart,\n                    //$.ArgumentStatementStart,\n                    $.StatementNumber,\n                    $.OrderedListItem,\n                    $.UnorderedListItem,\n                    //The ends of Bold and italic ranges need to be lexed before the starts\n                    $.AsteriskBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (** vs *)\n                    $.UnderscoreBoldEnd, //BoldEnd needs to be lexed before ItalicEnd (__ vs _)\n                    $.AsteriskItalicEnd,\n                    $.UnderscoreItalicEnd,\n                    //The starts of Bold and italic ranges need to be lexed after the ends\n                    $.AsteriskBoldStart, //BoldStart needs to be lexed before ItalicStart (** vs *)\n                    $.UnderscoreBoldStart, //BoldStart needs to be lexed before ItalicStart (__ vs _)\n                    $.AsteriskItalicStart,\n                    $.UnderscoreItalicStart,\n                    $.Link, //needs to be lexed before StatementReference\n                    $.Tag,\n                    // $.StatementDefinitionByNumber, // needs to be lexed before ArgumentReference\n                    // $.StatementReferenceByNumber, // needs to be lexed before ArgumentReference\n                    // $.StatementMentionByNumber, // needs to be lexed before ArgumentReference\n                    $.StatementDefinition,\n                    $.StatementReference,\n                    $.StatementMention,\n                    $.ArgumentDefinition,\n                    $.ArgumentReference,\n                    $.ArgumentMention,\n                    $.Newline,\n                    $.Spaces,\n                    $.Freestyle,\n                    $.UnusedControlChar\n                ],\n                \"inference_mode\": [\n                    $.Comment,\n                    $.InferenceEnd,\n                    $.MetadataStart,\n                    $.MetadataEnd,\n                    $.MetadataStatementEnd,\n                    $.ListDelimiter,\n                    $.Colon,\n                    $.Newline,\n                    $.Spaces,\n                    $.Freestyle,\n                    $.UnusedControlChar\n                ]\n            },\n\n            defaultMode: \"default_mode\"\n        };\n\n        this._lexer = new chevrotain.Lexer(lexerConfig);\n\n    }\n    tokensToString(tokens) {\n      let str = \"\";\n      for (let token of tokens) {\n          str += token.tokenType.tokenName + \" \" + token.image +\"\\n\";\n      }\n      return str;\n    }\n    tokenLocationsToString(tokens){\n        let str = \"\";\n        for (let token of tokens) {\n            str += token.tokenType.tokenName + \" \" + token.image + \"\\n\";\n            str += \"startOffset: \" + token.startOffset + \" endOffset: \" + token.endOffset +\" startLine: \"+token.startLine+\" endLine: \"+token.endLine+\" startColumn: \"+token.startColumn+\" endColumn: \"+token.endColumn+\"\\n\\n\";\n        }\n        return str;        \n    }\n    tokenize(text) {\n        this.init();\n\n        let lexResult = this._lexer.tokenize(text);\n        if (lexResult.errors.length > 0) {\n            throw new Error(\"sad sad panda lexing errors detected\");\n        }\n        \n        //remove trailing Emptyline (parser cannot cope with it)\n        if(tokenMatcher(_.last(lexResult.tokens), this.Emptyline)){\n          lexResult.tokens.pop();\n        }\n\n        this.emitRemainingDedentTokens(lexResult.tokens);\n\n        return lexResult;\n    }\n}\n\nmodule.exports = {\n    ArgdownLexer: new ArgdownLexer()\n};\n"]}